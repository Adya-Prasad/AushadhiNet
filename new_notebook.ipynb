{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "3b542dbe",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import json\n",
        "import time\n",
        "from datetime import timedelta, datetime\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch import amp\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import DataLoader  \n",
        "from torch_geometric.nn import GATv2Conv\n",
        "from torch_geometric.loader import LinkNeighborLoader\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import rdFingerprintGenerator, Descriptors\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, roc_auc_score,\n",
        "    classification_report, confusion_matrix\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "752d8fa4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "● DEVICE: cuda\n",
            "- GPU: NVIDIA GeForce GTX 1650\n",
            "- GPU Memory: 4.00 GB\n",
            "- ✓ Memory optimization enabled\n",
            "● Cleaning GPU Memory...\n",
            "- GPU Memory Freed. Allocated: 0.00 MB\n",
            "- Reserved: 0.00 MB\n"
          ]
        }
      ],
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"● DEVICE: {DEVICE}\")\n",
        "# Memory optimization settings for GPU, Aggressive memory management\n",
        "if DEVICE.type == 'cuda':\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "    # Force garbage collection\n",
        "    gc.collect()\n",
        "    print(f\"- GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"- GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "    print(\"- ✓ Memory optimization enabled\")\n",
        "\n",
        "# Force Garbage Collection\n",
        "print(\"● Cleaning GPU Memory...\")\n",
        "# Delete potential ghost variables if they exist\n",
        "try:\n",
        "    del model, optimizer, scaler, z, loss, out\n",
        "except NameError:\n",
        "    pass\n",
        "gc.collect()\n",
        "#  Clear CUDA Cache\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "    # Set Allocator Strategy to reduce fragmentation\n",
        "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "    print(f\"- GPU Memory Freed. Allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
        "    print(f\"- Reserved: {torch.cuda.memory_reserved()/1024**2:.2f} MB\")\n",
        "else:\n",
        "    print(\"-Running on CPU (No CUDA clear needed)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "57cd1315",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Notebook / plotting style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Device and basic GPU hygiene\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "    gc.collect()\n",
        "\n",
        "# Timestamp for saving artifacts\n",
        "TIMESTAMP = datetime.now().strftime(\"%d_%b_%H-%M\")\n",
        "\n",
        "# Ensure output folders exist\n",
        "for folder in [\"images\", \"models\"]:\n",
        "  os.makedirs(folder, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "4a3c5c75",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Data loaded successfully\n",
            "● DDI shape: (191808, 4)\n",
            "● SMILES shape: (1706, 2)\n",
            "● DDI columns: ['d1', 'd2', 'type', 'Neg samples']\n",
            "● SMILES columns: ['drug_id', 'smiles']\n",
            "\n",
            "ⓘ QUICK SANITY REPORT:\n",
            "\n",
            "[1. DDI Data]\n",
            "● rows: 191808\n",
            "● nulls per column: {'d1': 0, 'd2': 0, 'type': 0, 'Neg samples': 0}\n",
            "● unique types: 86\n",
            "● type distribution (top 10):\n",
            "type\n",
            "48    60751\n",
            "46    34360\n",
            "72    23779\n",
            "74     9470\n",
            "59     8397\n",
            "69     7786\n",
            "19     6140\n",
            "15     5413\n",
            "3      5011\n",
            "5      3160\n",
            "Name: count, dtype: int64\n",
            "\n",
            "[2. SMILES Data]\n",
            "● rows: 1706\n",
            "● nulls per column: {'drug_id': 0, 'smiles': 0}\n",
            "\n",
            "Overlap: 1706/1706 DDI drugs have SMILES (100.00%)\n"
          ]
        }
      ],
      "source": [
        "# Load DDI data\n",
        "\n",
        "ddi_data = 'dataset/drugdata/ddis.csv'\n",
        "drug_smiles_data = 'dataset/drugdata/drug_smiles.csv'\n",
        "if ddi_data and drug_smiles_data:\n",
        "    print(\"✓ Data loaded successfully\")\n",
        "def load_raw_data(ddi_path: str = ddi_data, smiles_path: str = drug_smiles_data):\n",
        "    \"\"\"Load raw DDI and SMILES tables from disk.\"\"\"\n",
        "    ddi_df = pd.read_csv(ddi_path)\n",
        "    smiles_df = pd.read_csv(smiles_path)\n",
        "\n",
        "    print(\"● DDI shape:\", ddi_df.shape)\n",
        "    print(\"● SMILES shape:\", smiles_df.shape)\n",
        "    print(\"● DDI columns:\", ddi_df.columns.tolist())\n",
        "    print(\"● SMILES columns:\", smiles_df.columns.tolist())\n",
        "\n",
        "    return ddi_df, smiles_df\n",
        "\n",
        "\n",
        "def quick_sanity_report(ddi_df: pd.DataFrame, smiles_df: pd.DataFrame):\n",
        "    \"\"\"Print a compact sanity report (non‑destructive).\"\"\"\n",
        "    print(\"\\nⓘ QUICK SANITY REPORT:\\n\")\n",
        "\n",
        "    # DDI\n",
        "    print(\"[1. DDI Data]\")\n",
        "    print(\"● rows:\", len(ddi_df))\n",
        "    print(\"● nulls per column:\", ddi_df.isnull().sum().to_dict())\n",
        "    if {\"d1\", \"d2\", \"type\"}.issubset(ddi_df.columns):\n",
        "        print(\"● unique types:\", ddi_df[\"type\"].nunique())\n",
        "        print(\"● type distribution (top 10):\")\n",
        "        print(ddi_df[\"type\"].value_counts().head(10))\n",
        "\n",
        "    # SMILES\n",
        "    print(\"\\n[2. SMILES Data]\")\n",
        "    print(\"● rows:\", len(smiles_df))\n",
        "    print(\"● nulls per column:\", smiles_df.isnull().sum().to_dict())\n",
        "\n",
        "    # Overlap\n",
        "    ddi_drugs = set(ddi_df[\"d1\"]).union(set(ddi_df[\"d2\"]))\n",
        "    smiles_drugs = set(smiles_df[\"drug_id\"]) if \"drug_id\" in smiles_df.columns else set()\n",
        "    overlap = ddi_drugs & smiles_drugs\n",
        "    print(f\"\\nOverlap: {len(overlap)}/{len(ddi_drugs)} DDI drugs have SMILES ({100*len(overlap)/len(ddi_drugs):.2f}%)\")\n",
        "\n",
        "\n",
        "# Load once (reused later)\n",
        "ddi_df, smiles_df = load_raw_data()\n",
        "quick_sanity_report(ddi_df, smiles_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "d887a856",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting RDKit features from SMILES…\n",
            "● 200/1706 processed\n",
            "● 400/1706 processed\n",
            "● 600/1706 processed\n",
            "● 800/1706 processed\n",
            "● 1000/1706 processed\n",
            "● 1200/1706 processed\n",
            "● 1400/1706 processed\n",
            "● 1600/1706 processed\n",
            "✓ Features for 1706 drugs (0 failures)\n",
            "\n",
            "Feature dim: 1032\n"
          ]
        }
      ],
      "source": [
        "# 3. RDKit‑based drug feature extraction\n",
        "\n",
        "class DrugFeatureExtractor:\n",
        "    \"\"\"SMILES → numeric feature vector using RDKit\n",
        "    Features:\n",
        "    - 1024 bit Morgan fingerprint (radius=2)\n",
        "    - A small set of physicochemical descriptors\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, fp_size: int = 1024, radius: int = 2):\n",
        "        self.fp_size = fp_size\n",
        "        self.radius = radius\n",
        "        self._mfpgen = rdFingerprintGenerator.GetMorganGenerator(radius=radius, fpSize=fp_size)\n",
        "\n",
        "    def smiles_to_features(self, smiles: str):\n",
        "        mol = Chem.MolFromSmiles(str(smiles))\n",
        "        if mol is None:\n",
        "            return None\n",
        "\n",
        "        # Fingerprint\n",
        "        fp = self._mfpgen.GetFingerprint(mol)\n",
        "        fp_array = np.array(fp, dtype=np.float32)\n",
        "\n",
        "        # Simple descriptors\n",
        "        desc = np.array([\n",
        "            Descriptors.MolWt(mol),\n",
        "            Descriptors.MolLogP(mol),\n",
        "            Descriptors.NumHDonors(mol),\n",
        "            Descriptors.NumHAcceptors(mol),\n",
        "            Descriptors.TPSA(mol),\n",
        "            Descriptors.NumRotatableBonds(mol),\n",
        "            Descriptors.NumAromaticRings(mol),\n",
        "            Descriptors.FractionCSP3(mol),\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        return np.concatenate([fp_array, desc])\n",
        "\n",
        "    def build_feature_table(self, smiles_df: pd.DataFrame):\n",
        "        \"\"\"Return: dict {drug_id -> feature_vector}, plus the final feature dim.\"\"\"\n",
        "        features = {}\n",
        "        failed = 0\n",
        "\n",
        "        print(\"\\nExtracting RDKit features from SMILES…\")\n",
        "        for i, row in smiles_df.iterrows():\n",
        "            drug_id = row[\"drug_id\"]\n",
        "            feats = self.smiles_to_features(row[\"smiles\"])\n",
        "            if feats is None:\n",
        "                failed += 1\n",
        "                continue\n",
        "            features[drug_id] = feats\n",
        "            if (i + 1) % 200 == 0:\n",
        "                print(f\"● {i+1}/{len(smiles_df)} processed\")\n",
        "\n",
        "        print(f\"✓ Features for {len(features)} drugs ({failed} failures)\")\n",
        "        feat_dim = len(next(iter(features.values())))\n",
        "        return features, feat_dim\n",
        "\n",
        "\n",
        "feature_extractor = DrugFeatureExtractor()\n",
        "drug_features, feature_dim = feature_extractor.build_feature_table(smiles_df)\n",
        "print(\"\\nFeature dim:\", feature_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "0c2bbed7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "● Positive directed edges: 383616\n",
            "● Negative directed edges (from dataset hints): 197610\n",
            "● interaction types: 86\n",
            "● Total edges: 581226 (pos+neg)\n"
          ]
        }
      ],
      "source": [
        "# 4. Graph construction with positive & negative edges\n",
        "class DDIGraph:\n",
        "    \"\"\"Utility to construct a PyG style graph for DDI.\n",
        "\n",
        "    - Nodes: drugs for which we have RDKit features.\n",
        "    - Positive edges: from `ddis.csv` (observed interactions).\n",
        "    - Negative edges: sampled using the `Neg samples` column when available;\n",
        "      we treat these as non interacting pairs.\n",
        "\n",
        "    Multi task targets per edge:\n",
        "    - `y_binary`: 0/1 for interaction existence.\n",
        "    - `y_type`: encoded interaction type for positives, -1 for negatives.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ddi_df: pd.DataFrame, drug_features: dict):\n",
        "        self.ddi_df = ddi_df\n",
        "        self.drug_features = drug_features\n",
        "        self.drug_to_idx = {}\n",
        "        self.idx_to_drug = {}\n",
        "        self.type_encoder = LabelEncoder()\n",
        "\n",
        "    def _index_drugs(self):\n",
        "        unique_drugs = sorted(self.drug_features.keys())\n",
        "        self.drug_to_idx = {d: i for i, d in enumerate(unique_drugs)}\n",
        "        self.idx_to_drug = {i: d for d, i in self.drug_to_idx.items()}\n",
        "        return unique_drugs\n",
        "\n",
        "    def build(self):\n",
        "        unique_drugs = self._index_drugs()\n",
        "        n_nodes = len(unique_drugs)\n",
        "\n",
        "        # Node feature matrix\n",
        "        feat_dim = len(next(iter(self.drug_features.values())))\n",
        "        x = np.zeros((n_nodes, feat_dim), dtype=np.float32)\n",
        "        for d, idx in self.drug_to_idx.items():\n",
        "            x[idx] = self.drug_features[d]\n",
        "\n",
        "        # Standardize features channel‑wise\n",
        "        x_mean = x.mean(axis=0, keepdims=True)\n",
        "        x_std = x.std(axis=0, keepdims=True) + 1e-8\n",
        "        x = (x - x_mean) / x_std\n",
        "\n",
        "        pos_edges = []\n",
        "        pos_types = []\n",
        "        neg_edges = []\n",
        "\n",
        "        # Positive edges from DDI table\n",
        "        for _, row in self.ddi_df.iterrows():\n",
        "            d1, d2 = row[\"d1\"], row[\"d2\"]\n",
        "            if d1 not in self.drug_to_idx or d2 not in self.drug_to_idx:\n",
        "                continue\n",
        "            u, v = self.drug_to_idx[d1], self.drug_to_idx[d2]\n",
        "            # bidirectional\n",
        "            pos_edges.extend([(u, v), (v, u)])\n",
        "            pos_types.extend([row[\"type\"], row[\"type\"]])\n",
        "\n",
        "            # Negatives from `Neg samples` if available\n",
        "            if \"Neg samples\" in row and isinstance(row[\"Neg samples\"], str):\n",
        "                neg_tokens = [t for t in row[\"Neg samples\"].split(\"$t\") if t]\n",
        "                for neg_d in neg_tokens:\n",
        "                    if neg_d in self.drug_to_idx:\n",
        "                        w = self.drug_to_idx[neg_d]\n",
        "                        neg_edges.extend([(w, v), (v, w)])\n",
        "\n",
        "        print(f\"● Positive directed edges: {len(pos_edges)}\")\n",
        "        print(f\"● Negative directed edges (from dataset hints): {len(neg_edges)}\")\n",
        "\n",
        "        # Encode interaction types (for positives only)\n",
        "        pos_types_encoded = self.type_encoder.fit_transform(np.array(pos_types))\n",
        "        n_types = len(self.type_encoder.classes_)\n",
        "        print(f\"● interaction types: {n_types}\")\n",
        "\n",
        "        # Build unified edge list and labels\n",
        "        edge_src = []\n",
        "        edge_dst = []\n",
        "        y_binary = []\n",
        "        y_type = []\n",
        "\n",
        "        # positives\n",
        "        for (u, v), t_enc in zip(pos_edges, pos_types_encoded):\n",
        "            edge_src.append(u)\n",
        "            edge_dst.append(v)\n",
        "            y_binary.append(1)\n",
        "            y_type.append(t_enc)\n",
        "\n",
        "        # negatives (type unknown / not applicable)\n",
        "        for (u, v) in neg_edges:\n",
        "            edge_src.append(u)\n",
        "            edge_dst.append(v)\n",
        "            y_binary.append(0)\n",
        "            y_type.append(-1)  # ignored in type loss\n",
        "\n",
        "        edge_index = torch.tensor([edge_src, edge_dst], dtype=torch.long)\n",
        "        y_binary = torch.tensor(y_binary, dtype=torch.long)\n",
        "        y_type = torch.tensor(y_type, dtype=torch.long)\n",
        "\n",
        "        x_tensor = torch.from_numpy(x).float()\n",
        "\n",
        "        print(f\"● Total edges: {edge_index.shape[1]} (pos+neg)\")\n",
        "\n",
        "        return {\n",
        "            \"x\": x_tensor,\n",
        "            \"edge_index\": edge_index,\n",
        "            \"y_binary\": y_binary,\n",
        "            \"y_type\": y_type,\n",
        "            \"n_nodes\": n_nodes,\n",
        "            \"n_types\": n_types,\n",
        "            \"type_encoder\": self.type_encoder,\n",
        "            \"drug_to_idx\": self.drug_to_idx,\n",
        "            \"idx_to_drug\": self.idx_to_drug,\n",
        "        }\n",
        "\n",
        "\n",
        "graph_builder = DDIGraph(ddi_df, drug_features)\n",
        "graph = graph_builder.build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "9159ab94",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "● Train edges: 406858 (70.0% of total)\n",
            "● Val edges: 87184 (15.0% of total)\n",
            "● Test edges: 87184 (15.0% of total)\n"
          ]
        }
      ],
      "source": [
        "# 5. Train/val/test split at edge level\n",
        "def split_edges(graph, val_size=0.15, test_size=0.15, random_state=42):\n",
        "    n_edges = graph[\"edge_index\"].shape[1]\n",
        "    all_idx = np.arange(n_edges)\n",
        "\n",
        "    train_idx, temp_idx = train_test_split(\n",
        "        all_idx, test_size=val_size + test_size, random_state=random_state, shuffle=True\n",
        "    )\n",
        "    val_rel = val_size / (val_size + test_size)\n",
        "    val_idx, test_idx = train_test_split(\n",
        "        temp_idx, test_size=1 - val_rel, random_state=random_state, shuffle=True\n",
        "    )\n",
        "\n",
        "    masks = {}\n",
        "    for name, idx in zip([\"train\", \"val\", \"test\"], [train_idx, val_idx, test_idx]):\n",
        "        m = torch.zeros(n_edges, dtype=torch.bool)\n",
        "        m[idx] = True\n",
        "        masks[f\"{name}_mask\"] = m\n",
        "        print(f\"● {name.capitalize()} edges: {m.sum().item()} ({100*m.float().mean():.1f}% of total)\")\n",
        "\n",
        "    return masks\n",
        "\n",
        "\n",
        "masks = split_edges(graph)\n",
        "graph.update(masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "9cdb1f07",
      "metadata": {},
      "outputs": [],
      "source": [
        "class GATv2DDI(nn.Module):\n",
        "    \"\"\"GATv2 for edge level DDI prediction with mutiple heads:\n",
        "    - binary (interaction yes/no)\n",
        "    - type (multi class), trained only on positive edges.\n",
        "\n",
        "    This version factorizes into an encoder (node level) and a decoder\n",
        "    (edge level) so we can cache node embeddings once per epoch during\n",
        "    training and reuse them for many edge mini batches.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_dim, hidden_dim, n_heads, n_types, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.gat1 = GATv2Conv(in_dim, hidden_dim, heads=n_heads, dropout=dropout, concat=True)\n",
        "        self.gat2 = GATv2Conv(hidden_dim * n_heads, hidden_dim, heads=n_heads, dropout=dropout, concat=True)\n",
        "        self.gat3 = GATv2Conv(hidden_dim * n_heads, hidden_dim, heads=1, dropout=dropout, concat=False)\n",
        "\n",
        "        node_emb_dim = hidden_dim  # output of gat3\n",
        "\n",
        "        # Edge MLP shared trunk\n",
        "        self.edge_mlp = nn.Sequential(\n",
        "            nn.Linear(node_emb_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        # Two task‑specific heads\n",
        "        self.binary_head = nn.Linear(hidden_dim // 2, 2)\n",
        "        self.type_head = nn.Linear(hidden_dim // 2, n_types)\n",
        "\n",
        "    def encode(self, x, edge_index):\n",
        "        \"\"\"Compute node embeddings for the whole graph.\"\"\"\n",
        "        x = x.float()\n",
        "        x = F.elu(self.gat1(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.elu(self.gat2(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.elu(self.gat3(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return x\n",
        "\n",
        "    def edge_forward(self, node_emb, edge_index_supervised):\n",
        "        \"\"\"Edge level prediction from pre-computed node embeddings.\"\"\"\n",
        "        src, dst = edge_index_supervised\n",
        "        h_src, h_dst = node_emb[src], node_emb[dst]\n",
        "        h_edge = torch.cat([h_src, h_dst], dim=-1)\n",
        "        h_shared = self.edge_mlp(h_edge)\n",
        "        logits_bin = self.binary_head(h_shared)\n",
        "        logits_type = self.type_head(h_shared)\n",
        "        return logits_bin, logits_type\n",
        "\n",
        "    def forward(self, x, edge_index, edge_index_supervised):\n",
        "        \"\"\"Standard full graph forward (kept for evaluation/inference).\"\"\"\n",
        "        node_emb = self.encode(x, edge_index)\n",
        "        return self.edge_forward(node_emb, edge_index_supervised)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "4ee303cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. Edge dataset and training utilities\n",
        "\n",
        "class EdgeDataset(Dataset):\n",
        "    def __init__(self, graph, mask_name: str):\n",
        "        mask = graph[mask_name]\n",
        "        self.edge_index = graph[\"edge_index\"][:, mask]\n",
        "        self.y_binary = graph[\"y_binary\"][mask]\n",
        "        self.y_type = graph[\"y_type\"][mask]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.edge_index.shape[1]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            self.edge_index[:, idx],\n",
        "            self.y_binary[idx],\n",
        "            self.y_type[idx],\n",
        "        )\n",
        "\n",
        "def compute_binary_metrics(y_true, y_prob):\n",
        "    y_pred = (y_prob[:, 1] >= 0.5).astype(int)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    try:\n",
        "        auc = roc_auc_score(y_true, y_prob[:, 1])\n",
        "    except ValueError:\n",
        "        auc = float(\"nan\")\n",
        "    return {\"acc\": acc, \"f1\": f1, \"auc\": auc}\n",
        "\n",
        "\n",
        "def compute_type_accuracy(y_type_true, y_type_pred):\n",
        "    mask = y_type_true >= 0\n",
        "    if mask.sum() == 0:\n",
        "        return 0.0\n",
        "    return accuracy_score(y_type_true[mask], y_type_pred[mask])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "ac8e3926",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. GATv2 multi‑task model (binary + type)\n",
        "\n",
        "class GATv2DDI(nn.Module):\n",
        "    \"\"\"GATv2 for edge‑level DDI prediction with two heads:\n",
        "    - binary (interaction yes/no)\n",
        "    - type (multi‑class), trained only on positive edges\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_dim, hidden_dim, n_heads, n_types, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.gat1 = GATv2Conv(in_dim, hidden_dim, heads=n_heads, dropout=dropout, concat=True)\n",
        "        self.gat2 = GATv2Conv(hidden_dim * n_heads, hidden_dim, heads=n_heads, dropout=dropout, concat=True)\n",
        "        self.gat3 = GATv2Conv(hidden_dim * n_heads, hidden_dim, heads=1, dropout=dropout, concat=False)\n",
        "\n",
        "        node_emb_dim = hidden_dim  # output of gat3\n",
        "\n",
        "        # Edge MLP shared trunk\n",
        "        self.edge_mlp = nn.Sequential(\n",
        "            nn.Linear(node_emb_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        # Two task‑specific heads\n",
        "        self.binary_head = nn.Linear(hidden_dim // 2, 2)\n",
        "        self.type_head = nn.Linear(hidden_dim // 2, n_types)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_index_supervised):\n",
        "        # Node encoder (full graph)\n",
        "        x = x.float()\n",
        "        x = F.elu(self.gat1(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        x = F.elu(self.gat2(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        x = F.elu(self.gat3(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Edge representation\n",
        "        src, dst = edge_index_supervised\n",
        "        h_src, h_dst = x[src], x[dst]\n",
        "        h_edge = torch.cat([h_src, h_dst], dim=-1)\n",
        "\n",
        "        h_shared = self.edge_mlp(h_edge)\n",
        "        logits_bin = self.binary_head(h_shared)\n",
        "        logits_type = self.type_head(h_shared)\n",
        "\n",
        "        return logits_bin, logits_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "62f3b99c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_history(history, save_path=None):\n",
        "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
        "\n",
        "    # Loss\n",
        "    axes[0].plot(epochs, history[\"train_loss\"], label=\"Train\")\n",
        "    axes[0].plot(epochs, history[\"val_loss\"], label=\"Val\")\n",
        "    axes[0].set_title(\"Loss\")\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Binary F1\n",
        "    axes[1].plot(epochs, history[\"train_bin_f1\"], label=\"Train F1\")\n",
        "    axes[1].plot(epochs, history[\"val_bin_f1\"], label=\"Val F1\")\n",
        "    axes[1].set_title(\"Binary F1\")\n",
        "    axes[1].legend()\n",
        "\n",
        "    # Binary AUC\n",
        "    axes[2].plot(epochs, history[\"val_bin_auc\"], label=\"Val AUC\")\n",
        "    axes[2].set_title(\"Validation AUC\")\n",
        "    axes[2].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if save_path is not None:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
        "        print(f\"History plot saved to {save_path}\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4194314a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8. Training loop (modular, with history tracking)\n",
        "def train_model(graph, config):\n",
        "    \"\"\"Full training loop. Returns (model, history dict)\"\"\"\n",
        "    x = graph[\"x\"].to(device)\n",
        "    full_edge_index = graph[\"edge_index\"].to(device)\n",
        "\n",
        "    model = GATv2DDI(\n",
        "        in_dim=x.shape[1],\n",
        "        hidden_dim=config[\"hidden_dim\"],\n",
        "        n_heads=config[\"n_heads\"],\n",
        "        n_types=graph[\"n_types\"],\n",
        "        dropout=config[\"dropout\"],\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
        "    scaler = amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
        "\n",
        "    # Losses\n",
        "    ce_binary = nn.CrossEntropyLoss()\n",
        "    ce_type = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "    # Datasets / loaders\n",
        "    train_ds = EdgeDataset(graph, \"train_mask\")\n",
        "    val_ds = EdgeDataset(graph, \"val_mask\")\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=config[\"batch_size\"], shuffle=False)\n",
        "\n",
        "    history = {\n",
        "        \"train_bin_acc\": [], \"train_bin_f1\": [], \"train_bin_auc\": [],\n",
        "        \"val_bin_acc\": [], \"val_bin_f1\": [], \"val_bin_auc\": [],\n",
        "        \"val_type_acc\": [],\n",
        "        \"train_loss\": [], \"val_loss\": [], \"training_duration\":[],\n",
        "    }\n",
        "\n",
        "    # Intializing some variables\n",
        "    start_time = time.perf_counter()\n",
        "    print(f\"Model Training started at: {time.strftime('%H:%M:%S', time.localtime())}...\\n\")\n",
        "\n",
        "    best_val_f1 = -1\n",
        "    best_state = None\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(1, config[\"epochs\"] + 1):\n",
        "        model.train()\n",
        "        epoch_losses = []\n",
        "        all_yb_true, all_yb_prob = [], []\n",
        "\n",
        "        for edges, yb, yt in train_loader:\n",
        "            # DataLoader stacks edges as [batch, 2]; transpose to [2, batch]\n",
        "            edges = edges.t().contiguous().to(device)\n",
        "            yb = yb.to(device)\n",
        "            yt = yt.to(device)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            # Mixed precision on GPU; FP32 on CPU\n",
        "            with amp.autocast(\"cuda\", enabled=(device.type == \"cuda\")):\n",
        "                logits_bin, logits_type = model(x, full_edge_index, edges)\n",
        "                loss_bin = ce_binary(logits_bin, yb)\n",
        "                loss_type = ce_type(logits_type, yt)\n",
        "                loss = loss_bin + config[\"lambda_type\"] * loss_type\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            epoch_losses.append(loss.item())\n",
        "\n",
        "            probs_bin = torch.softmax(logits_bin.detach(), dim=1).cpu().numpy()\n",
        "            all_yb_true.append(yb.cpu().numpy())\n",
        "            all_yb_prob.append(probs_bin)\n",
        "\n",
        "        all_yb_true = np.concatenate(all_yb_true)\n",
        "        all_yb_prob = np.concatenate(all_yb_prob)\n",
        "        train_metrics = compute_binary_metrics(all_yb_true, all_yb_prob)\n",
        "\n",
        "        history[\"train_loss\"].append(float(np.mean(epoch_losses)))\n",
        "        history[\"train_bin_acc\"].append(train_metrics[\"acc\"])\n",
        "        history[\"train_bin_f1\"].append(train_metrics[\"f1\"])\n",
        "        history[\"train_bin_auc\"].append(train_metrics[\"auc\"])\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        vb_true, vb_prob = [], []\n",
        "        vt_true, vt_pred = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for edges, yb, yt in val_loader:\n",
        "                edges = edges.t().contiguous().to(device)\n",
        "                yb = yb.to(device)\n",
        "                yt = yt.to(device)\n",
        "\n",
        "                logits_bin, logits_type = model(x, full_edge_index, edges)\n",
        "                loss_bin = ce_binary(logits_bin, yb)\n",
        "                loss_type = ce_type(logits_type, yt)\n",
        "                loss = loss_bin + config[\"lambda_type\"] * loss_type\n",
        "                val_losses.append(loss.item())\n",
        "\n",
        "                probs_bin = torch.softmax(logits_bin, dim=1).cpu().numpy()\n",
        "                vb_true.append(yb.cpu().numpy())\n",
        "                vb_prob.append(probs_bin)\n",
        "\n",
        "                vt_true.append(yt.cpu().numpy())\n",
        "                vt_pred.append(torch.argmax(logits_type, dim=1).cpu().numpy())\n",
        "\n",
        "        vb_true = np.concatenate(vb_true)\n",
        "        vb_prob = np.concatenate(vb_prob)\n",
        "        vb_metrics = compute_binary_metrics(vb_true, vb_prob)\n",
        "\n",
        "        vt_true = np.concatenate(vt_true)\n",
        "        vt_pred = np.concatenate(vt_pred)\n",
        "        vt_acc = compute_type_accuracy(vt_true, vt_pred)\n",
        "\n",
        "        history[\"val_loss\"].append(float(np.mean(val_losses)))\n",
        "        history[\"val_bin_acc\"].append(vb_metrics[\"acc\"])\n",
        "        history[\"val_bin_f1\"].append(vb_metrics[\"f1\"])\n",
        "        history[\"val_bin_auc\"].append(vb_metrics[\"auc\"])\n",
        "        history[\"val_type_acc\"].append(vt_acc)\n",
        "\n",
        "        plot_history(history, save_path=f\"images/DDI_GATv2_history_{TIMESTAMP}.png\")\n",
        "\n",
        "        # Early stopping on validation F1\n",
        "        if vb_metrics[\"f1\"] > best_val_f1 + 1e-4:\n",
        "            best_val_f1 = vb_metrics[\"f1\"]\n",
        "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
        "            patience_counter = 0\n",
        "            improved = \"*\"\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            improved = \"\"\n",
        "\n",
        "        if (epoch % 1 == 0):\n",
        "            print(\n",
        "                f\"Epoch {epoch:03d} | \"\n",
        "                f\"TRAIN: B_acc={train_metrics['acc']:.3f} F1={train_metrics['f1']:.3f} | \"\n",
        "                f\"VALIDATION: B_acc={vb_metrics['acc']:.3f} F1={vb_metrics['f1']:.3f} \"\n",
        "                f\"TypeAcc={vt_acc:.3f} ROC-AUC={vb_metrics['auc']:.3f} {improved}\"\n",
        "            )\n",
        "\n",
        "        if patience_counter >= config[\"patience\"]:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            total_seconds = end_time - start_time\n",
        "            formatted_time = str(timedelta(seconds=int(total_seconds)))\n",
        "            history[\"training_duration\"].append(formatted_time)\n",
        "            print(f\"{epoch} epochs training complete in {history[\"training_duration\"]} at {time.strftime('%H:%M:%S', time.localtime())}\")\n",
        "            break\n",
        "    end_time = time.perf_counter()\n",
        "\n",
        "    # 3. Calculate and show final time\n",
        "    total_seconds = end_time - start_time\n",
        "    formatted_time = str(timedelta(seconds=int(total_seconds))) \n",
        "    history[\"training_duration\"].append(formatted_time)\n",
        "    print(f\"{config[\"epochs\"]} epochs training complete in {history[\"training_duration\"]} at {time.strftime('%H:%M:%S', time.localtime())}\")\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "aadd2ebe",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9. History plotting and evaluation on test set\n",
        "def evaluate_on_test(graph, model):\n",
        "    x = graph[\"x\"].to(device)\n",
        "    full_edge_index = graph[\"edge_index\"].to(device)\n",
        "\n",
        "    test_ds = EdgeDataset(graph, \"test_mask\")\n",
        "    test_loader = DataLoader(test_ds, batch_size=4096, shuffle=False)\n",
        "\n",
        "    ce_binary = nn.CrossEntropyLoss()\n",
        "    ce_type = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "    model.eval()\n",
        "    test_losses = []\n",
        "    yb_true, yb_prob = [], []\n",
        "    yt_true, yt_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for edges, yb, yt in test_loader:\n",
        "            # [batch, 2] -> [2, batch]\n",
        "            edges = edges.t().contiguous().to(device)\n",
        "            yb = yb.to(device)\n",
        "            yt = yt.to(device)\n",
        "\n",
        "            logits_bin, logits_type = model(x, full_edge_index, edges)\n",
        "            loss_bin = ce_binary(logits_bin, yb)\n",
        "            loss_type = ce_type(logits_type, yt)\n",
        "            loss = loss_bin + loss_type\n",
        "            test_losses.append(loss.item())\n",
        "\n",
        "            probs_bin = torch.softmax(logits_bin, dim=1).cpu().numpy()\n",
        "            yb_true.append(yb.cpu().numpy())\n",
        "            yb_prob.append(probs_bin)\n",
        "\n",
        "            yt_true.append(yt.cpu().numpy())\n",
        "            yt_pred.append(torch.argmax(logits_type, dim=1).cpu().numpy())\n",
        "\n",
        "    yb_true = np.concatenate(yb_true)\n",
        "    yb_prob = np.concatenate(yb_prob)\n",
        "    bin_metrics = compute_binary_metrics(yb_true, yb_prob)\n",
        "\n",
        "    yt_true = np.concatenate(yt_true)\n",
        "    yt_pred = np.concatenate(yt_pred)\n",
        "    type_acc = compute_type_accuracy(yt_true, yt_pred)\n",
        "\n",
        "    print(\"\\nModel Evaluation Summary:\")\n",
        "    print(f\"● Binary: acc={bin_metrics['acc']:.3f}, f1={bin_metrics['f1']:.3f}, auc={bin_metrics['auc']:.3f}\")\n",
        "    print(f\"● Type accuracy (positives only): {type_acc:.3f}\")\n",
        "\n",
        "    return {\n",
        "        \"bin_metrics\": bin_metrics,\n",
        "        \"type_acc\": type_acc,\n",
        "        \"y_binary_true\": yb_true,\n",
        "        \"y_binary_prob\": yb_prob,\n",
        "        \"y_type_true\": yt_true,\n",
        "        \"y_type_pred\": yt_pred,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9665d7a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Training started at: 20:14:32\n"
          ]
        }
      ],
      "source": [
        "# 10. High‑level training run (config)\n",
        "\n",
        "config = {\n",
        "    \"hidden_dim\": 128,\n",
        "    \"n_heads\": 4,\n",
        "    \"dropout\": 0.3,\n",
        "    \"lr\": 1e-3,\n",
        "    \"weight_decay\": 5e-4,\n",
        "    \"batch_size\": 2048,\n",
        "    \"epochs\": 4, # currently 5 only for speed testing purpose. \n",
        "    \"patience\": 20,\n",
        "    \"lambda_type\": 1.0,  # weight for type loss vs binary loss\n",
        "}\n",
        "\n",
        "model, history = train_model(graph, config)\n",
        "\n",
        "plot_history(history, save_path=f\"images/DDI_GATv2_history_{TIMESTAMP}.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "259302bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_results = evaluate_on_test(graph, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5dff5d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_interaction(drug_a: str, drug_b: str, model, graph, threshold: float = 0.5):\n",
        "    \"\"\"Predict interaction (0/1), probability, and type for a pair of DrugBank IDs.\n",
        "\n",
        "    Returns a dict with:\n",
        "    - binary_label (0/1)\n",
        "    - binary_prob (probability of interaction=1)\n",
        "    - type_index (internal index or None)\n",
        "    - type_original (original `type` value from DDI, if available)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    x = graph[\"x\"].to(device)\n",
        "    full_edge_index = graph[\"edge_index\"].to(device)\n",
        "\n",
        "    if drug_a not in graph[\"drug_to_idx\"] or drug_b not in graph[\"drug_to_idx\"]:\n",
        "        raise ValueError(\"One or both drugs are unknown to the graph (no SMILES/features).\")\n",
        "\n",
        "    u = graph[\"drug_to_idx\"][drug_a]\n",
        "    v = graph[\"drug_to_idx\"][drug_b]\n",
        "\n",
        "    edge = torch.tensor([[u], [v]], dtype=torch.long, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits_bin, logits_type = model(x, full_edge_index, edge)\n",
        "        prob_bin = torch.softmax(logits_bin, dim=1).cpu().numpy()[0]\n",
        "        yb_prob = float(prob_bin[1])\n",
        "        yb_label = int(yb_prob >= threshold)\n",
        "\n",
        "        # Type prediction (best guess; meaningful mainly if binary says interact)\n",
        "        type_idx = int(torch.argmax(logits_type, dim=1).cpu().item())\n",
        "        type_original = int(graph[\"type_encoder\"].inverse_transform([type_idx])[0])\n",
        "\n",
        "    return {\n",
        "        \"drug_a\": drug_a,\n",
        "        \"drug_b\": drug_b,\n",
        "        \"binary_label\": yb_label,\n",
        "        \"binary_prob\": yb_prob,\n",
        "        \"type_index\": type_idx,\n",
        "        \"type_original\": type_original,\n",
        "    }\n",
        "\n",
        "\n",
        "# Example usage (adjust drug IDs to ones that exist in your CSVs):\n",
        "# result = predict_interaction(\"DB04571\", \"DB00460\", model, graph)\n",
        "# print(result)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
