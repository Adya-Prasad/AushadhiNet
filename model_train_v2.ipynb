{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Attention Networks (GAT) - Complete Professional Implementation\n",
    "\n",
    "This notebook provides a comprehensive implementation of Graph Attention Networks (GAT) based on the paper \"Graph Attention Networks\" by VeliÄkoviÄ‡ et al.\n",
    "\n",
    "### Key Features:\n",
    "- âœ… Multi-head attention mechanism\n",
    "- âœ… Professional training pipeline with early stopping\n",
    "- âœ… Comprehensive evaluation metrics\n",
    "- âœ… Visualization capabilities\n",
    "- âœ… Error handling and logging\n",
    "\n",
    "### Architecture Overview:\n",
    "GATs work on graph data where nodes represent entities and edges represent relationships. The attention mechanism allows nodes to focus on the most relevant neighbors, similar to transformers but adapted for graph structures.\n",
    "\n",
    "```bash\n",
    "# Custom implementation - EDGE CLASSIFICATION\n",
    "class GraphAttentionLayer:\n",
    "    - Manual attention computation\n",
    "    - Sparse edge-based processing\n",
    "    - Memory-efficient scatter operations\n",
    "    - Gradient checkpointing\n",
    "    \n",
    "class GAT_DDI:\n",
    "    - 3 GAT layers (256 â†’ 256 â†’ 128 hidden dims)\n",
    "    - Edge MLP: 256 â†’ 128 â†’ 64 â†’ 86 classes\n",
    "    - Processes: node_features â†’ embeddings â†’ edge_features â†’ 86-way classification\n",
    "```\n",
    "```bash\n",
    "# For EACH batch of 7000 edges:\n",
    "1. Forward pass through 3 custom GAT layers\n",
    "2. Extract source/dest node embeddings\n",
    "3. Concatenate edge features\n",
    "4. Pass through 4-layer MLP\n",
    "5. Compute CrossEntropyLoss over 86 classes\n",
    "6. Backward pass with gradient checkpointing\n",
    "7. Repeat for ~27 batches per epoch\n",
    "\n",
    "# Per epoch: ~27 batches Ã— complex operations = SLOW\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "from safetensors.torch import save_file, load_file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator, Descriptors\n",
    "import networkx as nx\n",
    "from datetime import datetime\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce GTX 1650\n",
      "GPU Memory: 4.00 GB\n",
      "Memory optimization enabled âœ“\n",
      "Directory `images/` exists âœ“\n",
      "Directory `models/` exists âœ“\n",
      "Directory `results/` exists âœ“\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "# Memory optimization settings for GPU, Aggressive memory management\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(\"Memory optimization enabled âœ“\")\n",
    "\n",
    "# specific style date time saving\n",
    "timestamp = datetime.now().strftime(\"%d_%b_%H-%M\")\n",
    "\n",
    "# validating required directories:\n",
    "required_directories = ['images', 'models', 'results']\n",
    "for folder in required_directories:\n",
    "    if not os.path.exists(folder):\n",
    "        print(f\"Directory `{folder}/` not found âœ˜ Creating...\")\n",
    "        os.makedirs(folder)\n",
    "    else:\n",
    "        print(f\"Directory `{folder}/` exists âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPLORING DRUG-DRUG INTERACTION DATASET\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š DDI Dataset Shape: (191808, 4)\n",
      "Columns: ['d1', 'd2', 'type', 'Neg samples']\n",
      "\n",
      "ðŸ”¬ Interaction Types Distribution:\n",
      "type\n",
      "48    60751\n",
      "46    34360\n",
      "72    23779\n",
      "74     9470\n",
      "59     8397\n",
      "      ...  \n",
      "42       11\n",
      "61       11\n",
      "51       10\n",
      "25        7\n",
      "41        6\n",
      "Name: count, Length: 86, dtype: int64\n",
      "\n",
      "ðŸ“‹ Sample DDI Data:\n",
      "        d1       d2  type Neg samples\n",
      "0  DB04571  DB00460     0   DB01579$t\n",
      "1  DB00855  DB00460     0   DB01178$t\n",
      "2  DB09536  DB00460     0   DB06626$t\n",
      "3  DB01600  DB00460     0   DB01588$t\n",
      "4  DB09000  DB00460     0   DB06196$t\n",
      "5  DB11630  DB00460     0   DB00744$t\n",
      "6  DB00553  DB00460     0   DB06413$t\n",
      "7  DB06261  DB00460     0   DB00876$t\n",
      "8  DB01878  DB00460     0   DB09267$t\n",
      "9  DB00140  DB00460     0   DB01204$t\n",
      "\n",
      "ðŸ’Š Drug SMILES Dataset Shape: (1706, 2)\n",
      "Columns: ['drug_id', 'smiles']\n",
      "\n",
      "ðŸ“‹ Sample SMILES Data:\n",
      "   drug_id                                             smiles\n",
      "0  DB04571                CC1=CC2=CC3=C(OC(=O)C=C3C)C(C)=C2O1\n",
      "1  DB00855                                    NCC(=O)CCC(O)=O\n",
      "2  DB09536                                           O=[Ti]=O\n",
      "3  DB01600              CC(C(O)=O)C1=CC=C(S1)C(=O)C1=CC=CC=C1\n",
      "4  DB09000         CC(CN(C)C)CN1C2=CC=CC=C2SC2=C1C=C(C=C2)C#N\n",
      "5  DB11630  Oc1cccc(-c2c3nc(c(-c4cccc(O)c4)c4ccc([nH]4)c(-...\n",
      "6  DB00553                     COC1=C2OC(=O)C=CC2=CC2=C1OC=C2\n",
      "7  DB06261                      [H]N([H])CC(=O)CCC(=O)OCCCCCC\n",
      "8  DB01878                        O=C(C1=CC=CC=C1)C1=CC=CC=C1\n",
      "9  DB00140  CC1=C(C)C=C2N(C[C@H](O)[C@H](O)[C@H](O)CO)C3=N...\n",
      "\n",
      "ðŸ“ˆ Statistics:\n",
      "â€¢ Total DDI pairs: 191808\n",
      "â€¢ Unique drugs in DDI: 1706\n",
      "â€¢ Drugs with SMILES: 1706\n",
      "â€¢ Interaction type 0: 11\n",
      "â€¢ Interaction type 1: 323\n",
      "â€¢ Drugs with both DDI and SMILES: 1706\n",
      "â€¢ Coverage: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Load DDI data\n",
    "print(\"=\"*60)\n",
    "print(\"EXPLORING DRUG-DRUG INTERACTION DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load DDI interactions\n",
    "ddi_df = pd.read_csv('dataset/drugdata/ddis.csv')\n",
    "print(f\"\\nðŸ“Š DDI Dataset Shape: {ddi_df.shape}\")\n",
    "print(f\"Columns: {ddi_df.columns.tolist()}\")\n",
    "print(f\"\\nðŸ”¬ Interaction Types Distribution:\")\n",
    "print(ddi_df['type'].value_counts())\n",
    "print(f\"\\nðŸ“‹ Sample DDI Data:\")\n",
    "print(ddi_df.head(10))\n",
    "\n",
    "# Load drug SMILES\n",
    "smiles_df = pd.read_csv('dataset/drugdata/drug_smiles.csv')\n",
    "print(f\"\\nðŸ’Š Drug SMILES Dataset Shape: {smiles_df.shape}\")\n",
    "print(f\"Columns: {smiles_df.columns.tolist()}\")\n",
    "print(f\"\\nðŸ“‹ Sample SMILES Data:\")\n",
    "print(smiles_df.head(10))\n",
    "\n",
    "# Get unique drugs\n",
    "unique_drugs_ddi = set(ddi_df['d1'].unique()) | set(ddi_df['d2'].unique())\n",
    "print(f\"\\nðŸ“ˆ Statistics:\")\n",
    "print(f\"â€¢ Total DDI pairs: {len(ddi_df)}\")\n",
    "print(f\"â€¢ Unique drugs in DDI: {len(unique_drugs_ddi)}\")\n",
    "print(f\"â€¢ Drugs with SMILES: {len(smiles_df)}\")\n",
    "print(f\"â€¢ Interaction type 0: {(ddi_df['type'] == 0).sum()}\")\n",
    "print(f\"â€¢ Interaction type 1: {(ddi_df['type'] == 1).sum()}\")\n",
    "\n",
    "# Check overlap\n",
    "drugs_with_smiles = set(smiles_df['drug_id'].unique())\n",
    "overlap = unique_drugs_ddi & drugs_with_smiles\n",
    "print(f\"â€¢ Drugs with both DDI and SMILES: {len(overlap)}\")\n",
    "print(f\"â€¢ Coverage: {len(overlap)/len(unique_drugs_ddi)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ddi_data():\n",
    "    \"\"\"\n",
    "    Load and prepare DDI data\n",
    "    \"\"\"\n",
    "    print(\"_\"*40)\n",
    "    print(\"LOADING DRUG-DRUG INTERACTION DATA\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Load data\n",
    "    ddi_df = pd.read_csv('dataset/drugdata/ddis.csv')\n",
    "    smiles_df = pd.read_csv('dataset/drugdata/drug_smiles.csv')\n",
    "    \n",
    "    print(f\"âœ“ Loaded {len(ddi_df)} DDI pairs\")\n",
    "    print(f\"âœ“ Loaded {len(smiles_df)} drug SMILES\")\n",
    "    print(\"_\"*40)\n",
    "    \n",
    "    # Extract features\n",
    "    feature_extractor = DrugFeatureExtractor()\n",
    "    drug_features = feature_extractor.extract_all_features(smiles_df)\n",
    "    \n",
    "    # Build graph\n",
    "    graph_builder = DDIGraphBuilder(ddi_df, drug_features)\n",
    "    node_features, edge_index, edge_labels, n_classes = graph_builder.build_graph()\n",
    "    \n",
    "    # Create adjacency matrix\n",
    "    n_nodes = node_features.shape[0]\n",
    "    adj_mat = torch.zeros(n_nodes, n_nodes, 1)\n",
    "    adj_mat[edge_index[0], edge_index[1], 0] = 1\n",
    "    \n",
    "    # Add self-loops\n",
    "    adj_mat[range(n_nodes), range(n_nodes), 0] = 1\n",
    "    \n",
    "    # Split data\n",
    "    n_edges = edge_index.shape[1]\n",
    "    indices = np.arange(n_edges)\n",
    "    \n",
    "    train_idx, temp_idx = train_test_split(indices, test_size=0.3, random_state=42)\n",
    "    val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n",
    "    \n",
    "    train_mask = torch.zeros(n_edges, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(n_edges, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(n_edges, dtype=torch.bool)\n",
    "    \n",
    "    train_mask[train_idx] = True\n",
    "    val_mask[val_idx] = True\n",
    "    test_mask[test_idx] = True\n",
    "    \n",
    "    print(f\"\\nðŸ›ˆ Data Split:\")\n",
    "    print(f\"â€¢ Train: {train_mask.sum()} edges ({train_mask.sum()/n_edges*100:.1f}%)\")\n",
    "    print(f\"â€¢ Val: {val_mask.sum()} edges ({val_mask.sum()/n_edges*100:.1f}%)\")\n",
    "    print(f\"â€¢ Test: {test_mask.sum()} edges ({test_mask.sum()/n_edges*100:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'node_features': node_features,\n",
    "        'edge_index': edge_index,\n",
    "        'edge_labels': edge_labels,\n",
    "        'adj_mat': adj_mat,\n",
    "        'train_mask': train_mask,\n",
    "        'val_mask': val_mask,\n",
    "        'test_mask': test_mask,\n",
    "        'n_classes': n_classes,\n",
    "        'label_encoder': graph_builder.label_encoder\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature and Pattern Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrugFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extract molecular features from SMILES strings using RDKit\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def smiles_to_features(self, smiles):\n",
    "        \"\"\"\n",
    "        Convert SMILES to molecular fingerprint and descriptors\n",
    "        \n",
    "        Args:\n",
    "            smiles: SMILES string\n",
    "            \n",
    "        Returns:\n",
    "            Feature vector\n",
    "        \"\"\"\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                return None\n",
    "            \n",
    "            # Morgan fingerprint\n",
    "            mfpgen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=1024)\n",
    "            fp = fp = mfpgen.GetFingerprint(mol)\n",
    "            fp_array = np.array(fp)\n",
    "            \n",
    "            # Molecular descriptors\n",
    "            descriptors = [\n",
    "                Descriptors.MolWt(mol),\n",
    "                Descriptors.MolLogP(mol),\n",
    "                Descriptors.NumHDonors(mol),\n",
    "                Descriptors.NumHAcceptors(mol),\n",
    "                Descriptors.TPSA(mol),\n",
    "                Descriptors.NumRotatableBonds(mol),\n",
    "                Descriptors.NumAromaticRings(mol),\n",
    "                Descriptors.FractionCSP3(mol),\n",
    "            ]\n",
    "            \n",
    "            # Combine fingerprint and descriptors\n",
    "            features = np.concatenate([fp_array, descriptors])\n",
    "            \n",
    "            return features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing SMILES: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_all_features(self, smiles_df):\n",
    "        \"\"\"\n",
    "        Extract features for all drugs\n",
    "        \n",
    "        Args:\n",
    "            smiles_df: DataFrame with drug_id and smiles columns\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping drug_id to feature vector\n",
    "        \"\"\"\n",
    "        print(\"\\n\"+\"_\"*45)\n",
    "        print(\"Extracting molecular features from SMILES...\")\n",
    "        print(\"-\"*45)\n",
    "\n",
    "        drug_features = {}\n",
    "        failed = 0\n",
    "        \n",
    "        for idx, row in smiles_df.iterrows():\n",
    "            drug_id = row['drug_id']\n",
    "            smiles = row['smiles']\n",
    "            \n",
    "            features = self.smiles_to_features(smiles)\n",
    "            \n",
    "            if features is not None:\n",
    "                drug_features[drug_id] = features\n",
    "            else:\n",
    "                failed += 1\n",
    "            \n",
    "            if (idx + 1) % 200 == 0:\n",
    "                print(f\"{idx + 1}/{len(smiles_df)} drugs processed \")\n",
    "        print(\"-\"*45)\n",
    "        print(f\"âœ“ Extracted features for {len(drug_features)} drugs\")\n",
    "        if failed > 0:\n",
    "            print(f\"âš  Failed to process {failed} drugs\")\n",
    "        print(\"_\"*45 + \"\\n\")\n",
    "        return drug_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GAT Architecture Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDIGraphBuilder:\n",
    "    \"\"\"\n",
    "    Build graph structure from DDI data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ddi_df, drug_features):\n",
    "        self.ddi_df = ddi_df\n",
    "        self.drug_features = drug_features\n",
    "        self.drug_to_idx = {}\n",
    "        self.idx_to_drug = {}\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def build_graph(self):\n",
    "        \"\"\"\n",
    "        Build graph from DDI data\n",
    "        \n",
    "        Returns:\n",
    "            Node features, edge indices, edge labels\n",
    "        \"\"\"\n",
    "        print(\"_\"*40)\n",
    "        print(\"Building DDI graph...\")\n",
    "        \n",
    "        # Get unique drugs\n",
    "        unique_drugs = sorted(list(self.drug_features.keys()))\n",
    "        self.drug_to_idx = {drug: idx for idx, drug in enumerate(unique_drugs)}\n",
    "        self.idx_to_drug = {idx: drug for drug, idx in self.drug_to_idx.items()}\n",
    "        \n",
    "        print(f\"â€¢ Number of nodes (drugs): {len(unique_drugs)}\")\n",
    "        \n",
    "        # Create node feature matrix\n",
    "        feature_dim = len(list(self.drug_features.values())[0])\n",
    "        node_features = np.zeros((len(unique_drugs), feature_dim))\n",
    "        \n",
    "        for drug, idx in self.drug_to_idx.items():\n",
    "            node_features[idx] = self.drug_features[drug]\n",
    "        \n",
    "        # Normalize features\n",
    "        node_features = (node_features - node_features.mean(axis=0)) / (node_features.std(axis=0) + 1e-8)\n",
    "        \n",
    "        # Build edges from DDI data\n",
    "        edge_list = []\n",
    "        edge_labels = []\n",
    "        \n",
    "        for _, row in self.ddi_df.iterrows():\n",
    "            d1, d2, interaction_type = row['d1'], row['d2'], row['type']\n",
    "            \n",
    "            if d1 in self.drug_to_idx and d2 in self.drug_to_idx:\n",
    "                idx1 = self.drug_to_idx[d1]\n",
    "                idx2 = self.drug_to_idx[d2]\n",
    "                \n",
    "                # Add bidirectional edges\n",
    "                edge_list.append([idx1, idx2])\n",
    "                edge_list.append([idx2, idx1])\n",
    "                edge_labels.append(interaction_type)\n",
    "                edge_labels.append(interaction_type)\n",
    "        \n",
    "        edge_index = torch.tensor(edge_list, dtype=torch.long).t()\n",
    "        \n",
    "        # Encode labels\n",
    "        edge_labels = self.label_encoder.fit_transform(edge_labels)\n",
    "        edge_labels = torch.tensor(edge_labels, dtype=torch.long)\n",
    "        \n",
    "        print(f\"â€¢ Number of edges: {len(edge_list)}\")\n",
    "        print(f\"â€¢ Number of interaction types: {len(self.label_encoder.classes_)}\")\n",
    "        print(f\"â€¢ Feature dimension: {feature_dim}\")\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(node_features),\n",
    "            edge_index,\n",
    "            edge_labels,\n",
    "            len(self.label_encoder.classes_)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Graph Attention Layer Implementation\n",
    "    \n",
    "    Args:\n",
    "        in_features: Number of input features per node (F)\n",
    "        out_features: Number of output features per node (F')\n",
    "        n_heads: Number of attention heads (K)\n",
    "        is_concat: Whether to concatenate or average multi-head results\n",
    "        dropout: Dropout probability for regularization\n",
    "        leaky_relu_negative_slope: Negative slope for LeakyReLU activation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int, n_heads: int=8, \n",
    "                 is_concat: bool = True, dropout: float = 0.6, \n",
    "                 leaky_relu_negative_slope: float = 0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.is_concat = is_concat\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        # Calculate dimensions per head\n",
    "        if is_concat:\n",
    "            assert out_features % n_heads == 0, \"out_features must be divisible by n_heads when concatenating\"\n",
    "            self.n_hidden = out_features // n_heads\n",
    "        else:\n",
    "            self.n_hidden = out_features\n",
    "        \n",
    "        # Linear transformation: W in the paper\n",
    "        self.linear = nn.Linear(in_features, self.n_hidden * n_heads, bias=False)\n",
    "        \n",
    "        # Attention parameters (a^T) (one per head)\n",
    "        self.attn_src = nn.Parameter(torch.Tensor(1, n_heads, self.n_hidden))\n",
    "        self.attn_dst = nn.Parameter(torch.Tensor(1, n_heads, self.n_hidden))\n",
    "        \n",
    "        # Activation and normalization\n",
    "        self.activation = nn.LeakyReLU(negative_slope=leaky_relu_negative_slope)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize weights using Xavier uniform\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize layer weights using Xavier uniform initialization\"\"\"\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        nn.init.xavier_uniform_(self.attn_src)\n",
    "        nn.init.xavier_uniform_(self.attn_dst)\n",
    "    \n",
    "    def forward(self, h: torch.Tensor, adj_mat: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the attention layer\n",
    "        \n",
    "        Args:\n",
    "            h: Node embeddings [n_nodes, in_features]\n",
    "            adj_mat: Adjacency matrix [n_nodes, n_nodes, n_heads]\n",
    "            \n",
    "        Returns:\n",
    "            Updated node embeddings [n_nodes, out_features]\n",
    "        \"\"\"\n",
    "        n_nodes = h.shape[0]\n",
    "        device = h.device\n",
    "        \n",
    "        # Step 1: Linear transformation g_i = W * h_i for each head\n",
    "        g = self.linear(h).view(n_nodes, self.n_heads, self.n_hidden)\n",
    "\n",
    "        # Extract edge indices from adjacency matrix (ony non-zero entries)\n",
    "        adj_2d = adj_mat.squeeze(-1)\n",
    "        edge_index = adj_2d.nonzero(as_tuple=False).t()\n",
    "\n",
    "        if edge_index.shape[1] == 0:\n",
    "            # No edges case\n",
    "            if self.is_concat:\n",
    "                return torch.zeros(n_nodes, self.n_heads * self.n_hidden, device=device)\n",
    "            else:\n",
    "                return torch.zeros(n_nodes, self.n_hidden, device=device)\n",
    "        \n",
    "        src_nodes = edge_index[0]\n",
    "        dst_nodes = edge_index[1]\n",
    "        \n",
    "        # Get features for source and destination nodes\n",
    "        g_src = g[src_nodes]  # [n_edges, n_heads, n_hidden]\n",
    "        g_dst = g[dst_nodes]  # [n_edges, n_heads, n_hidden]\n",
    "        \n",
    "        # Compute attention scores (edge-wise, memory efficient)\n",
    "        e_src = (g_src * self.attn_src).sum(dim=-1)  # [n_edges, n_heads]\n",
    "        e_dst = (g_dst * self.attn_dst).sum(dim=-1)  # [n_edges, n_heads]\n",
    "        e = self.activation(e_src + e_dst)  # [n_edges, n_heads]\n",
    "        \n",
    "        # Softmax normalization per destination node\n",
    "        # Group by destination node for proper normalization\n",
    "        alpha = torch.zeros_like(e)\n",
    "        # Process each head separately to save memory\n",
    "        for head in range(self.n_heads):\n",
    "            e_head = e[:, head]\n",
    "            \n",
    "            # Use scatter operations instead of loops\n",
    "            # Create index for scatter\n",
    "            max_dst = dst_nodes.max().item() + 1\n",
    "            \n",
    "            # Compute max for numerical stability\n",
    "            max_vals = torch.full((max_dst,), float('-inf'), device=device)\n",
    "            max_vals.scatter_reduce_(0, dst_nodes, e_head, reduce='amax', include_self=False)\n",
    "            \n",
    "            # Subtract max and exp\n",
    "            e_stable = e_head - max_vals[dst_nodes]\n",
    "            exp_e = torch.exp(e_stable)\n",
    "            \n",
    "            # Sum exp values per destination\n",
    "            sum_exp = torch.zeros(max_dst, device=device)\n",
    "            sum_exp.scatter_add_(0, dst_nodes, exp_e)\n",
    "            \n",
    "            # Normalize\n",
    "            alpha[:, head] = exp_e / (sum_exp[dst_nodes] + 1e-16)\n",
    "        \n",
    "        # Aggregate features using attention weights\n",
    "        out = torch.zeros(n_nodes, self.n_heads, self.n_hidden, device=device)\n",
    "        \n",
    "        # Efficient aggregation using scatter_add\n",
    "        for head in range(self.n_heads):\n",
    "            # Weighted features for this head\n",
    "            weighted_features = g_src[:, head, :] * alpha[:, head].unsqueeze(-1)\n",
    "            \n",
    "            # Aggregate to destination nodes\n",
    "            out[:, head, :].scatter_add_(0, \n",
    "                                         dst_nodes.unsqueeze(-1).expand(-1, self.n_hidden),\n",
    "                                         weighted_features)\n",
    "        \n",
    "        # Concatenate or average heads\n",
    "        if self.is_concat:\n",
    "            return out.reshape(n_nodes, self.n_heads * self.n_hidden)\n",
    "        else:\n",
    "            return out.mean(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT_DDI(nn.Module):\n",
    "    \"\"\"\n",
    "    Memory-efficient GAT with Mixed Precision support using PyTorch Geometric's GATv2Conv\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_features, n_hidden, n_classes, n_heads=8, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Replace custom GAT with optimized GATv2Conv\n",
    "        self.gat1 = GATv2Conv(n_features, n_hidden, heads=n_heads, dropout=dropout, concat=True)\n",
    "        self.gat2 = GATv2Conv(n_hidden * n_heads, n_hidden, heads=n_heads, dropout=dropout, concat=True)\n",
    "        self.gat3 = GATv2Conv(n_hidden * n_heads, n_hidden // 2, heads=n_heads, dropout=dropout, concat=True)\n",
    "        \n",
    "        # Input: concatenated embeddings from src and dst nodes\n",
    "        edge_input_dim = (n_hidden // 2) * n_heads * 2  # *2 because we concatenate src and dst\n",
    "        \n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(edge_input_dim, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(n_hidden, n_hidden // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(n_hidden // 2, n_classes)\n",
    "        )\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_index_for_prediction):\n",
    "        \"\"\"\n",
    "        Forward pass using edge_index directly (no adjacency matrix)\n",
    "        \n",
    "        Args:\n",
    "            x: Node features [n_nodes, n_features]\n",
    "            edge_index: Graph connectivity [2, n_edges] - for message passing\n",
    "            edge_index_for_prediction: Edges to classify [2, n_prediction_edges]\n",
    "        \n",
    "        Returns:\n",
    "            Edge predictions [n_prediction_edges, n_classes]\n",
    "        \"\"\"\n",
    "        # âœ… FP16 FIX: Ensure inputs are float for mixed precision\n",
    "        x = x.float()\n",
    "        \n",
    "        # Layer 1\n",
    "        x = self.dropout_layer(x)\n",
    "        x = F.elu(self.gat1(x, edge_index))\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.dropout_layer(x)\n",
    "        x = F.elu(self.gat2(x, edge_index))\n",
    "        \n",
    "        # Layer 3\n",
    "        x = self.dropout_layer(x)\n",
    "        x = F.elu(self.gat3(x, edge_index))\n",
    "        \n",
    "        # Edge classification\n",
    "        src = edge_index_for_prediction[0]\n",
    "        dst = edge_index_for_prediction[1]\n",
    "        edge_features = torch.cat([x[src], x[dst]], dim=1)\n",
    "        \n",
    "        return self.edge_mlp(edge_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Pipeline\n",
    "\n",
    "Our training pipeline includes:\n",
    "- Early stopping to prevent overfitting\n",
    "- Learning rate scheduling\n",
    "- Comprehensive logging\n",
    "- Model checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDITrainer:\n",
    "    \"\"\"\n",
    "    Memory-efficient trainer with Mixed Precision Training (FP16)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, device):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.history = {\n",
    "            'train_loss': [], 'val_loss': [],\n",
    "            'train_acc': [], 'val_acc': [],\n",
    "            'train_f1': [], 'val_f1': []\n",
    "        }\n",
    "        \n",
    "        # Initialize GradScaler for mixed precision\n",
    "        self.scaler = GradScaler('cuda')\n",
    "    \n",
    "    def train_epoch(self, x, edge_index, edge_labels, train_mask, optimizer, criterion, batch_size=512):\n",
    "        \"\"\"Train for one epoch with Mixed Precision Training\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        train_indices = torch.where(train_mask)[0]\n",
    "        n_train = len(train_indices)\n",
    "        \n",
    "        n_batches = (n_train + batch_size - 1) // batch_size\n",
    "        \n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        # Shuffle training indices\n",
    "        perm = torch.randperm(n_train, device=self.device)\n",
    "        train_indices = train_indices[perm]\n",
    "\n",
    "        # Track time for progress reporting\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, n_train)\n",
    "            batch_indices = train_indices[start_idx:end_idx]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get batch edges\n",
    "            batch_edge_index = edge_index[:, batch_indices]\n",
    "            batch_labels = edge_labels[batch_indices]\n",
    "            \n",
    "            # Get unique nodes\n",
    "            unique_nodes = torch.unique(batch_edge_index.flatten())\n",
    "            n_batch_nodes = len(unique_nodes)\n",
    "            \n",
    "            # âœ… SIMPLIFIED: Direct subgraph without complex neighbor sampling\n",
    "            # For efficiency, just use the batch nodes + immediate neighbors\n",
    "            node_mapping = torch.full((x.shape[0],), -1, dtype=torch.long, device=self.device)\n",
    "            node_mapping[unique_nodes] = torch.arange(n_batch_nodes, device=self.device)\n",
    "            \n",
    "            # Extract subgraph features\n",
    "            subgraph_x = x[unique_nodes]\n",
    "            \n",
    "            # Get edges within the subgraph\n",
    "            node_mask = torch.zeros(x.shape[0], dtype=torch.bool, device=self.device)\n",
    "            node_mask[unique_nodes] = True\n",
    "            edge_mask = node_mask[edge_index[0]] & node_mask[edge_index[1]]\n",
    "            subgraph_edges = edge_index[:, edge_mask]\n",
    "            subgraph_edge_index = node_mapping[subgraph_edges]\n",
    "            \n",
    "            # Remap batch edges\n",
    "            batch_edge_index_remapped = node_mapping[batch_edge_index]\n",
    "            \n",
    "            # Mixed Precision Forward\n",
    "            with autocast('cuda'):\n",
    "                out = self.model(subgraph_x, subgraph_edge_index, batch_edge_index_remapped)\n",
    "                loss = criterion(out, batch_labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.scaler.step(optimizer)\n",
    "            self.scaler.update()\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            total_loss += loss.item() * len(batch_indices)\n",
    "            pred = out.float().max(1)[1]\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "            \n",
    "            # âœ… REDUCED: Clear cache less frequently (every 20 batches instead of 2)\n",
    "            if (i + 1) % 20 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "                # âœ… NEW: Print progress during epoch\n",
    "                elapsed = time.time() - epoch_start\n",
    "                eta = elapsed / (i + 1) * (n_batches - i - 1)\n",
    "                print(f\"\\r    Batch {i+1}/{n_batches} | Loss: {loss.item():.4f} | \"\n",
    "                      f\"Elapsed: {elapsed:.0f}s | ETA: {eta:.0f}s\", end=\"\", flush=True)\n",
    "        \n",
    "        print()  # New line after progress\n",
    "        \n",
    "        avg_loss = total_loss / n_train\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        return avg_loss, acc, f1\n",
    "    \n",
    "    def validate(self, x, edge_index, edge_labels, val_mask, criterion, batch_size=1024):\n",
    "        \"\"\"Fast validation without neighbor sampling complexity\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        val_indices = torch.where(val_mask)[0]\n",
    "        n_val = len(val_indices)\n",
    "        n_batches = (n_val + batch_size - 1) // batch_size\n",
    "        \n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(n_batches):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min((i + 1) * batch_size, n_val)\n",
    "                batch_indices = val_indices[start_idx:end_idx]\n",
    "                \n",
    "                batch_edge_index = edge_index[:, batch_indices]\n",
    "                batch_labels = edge_labels[batch_indices]\n",
    "                \n",
    "                # Simple subgraph extraction\n",
    "                unique_nodes = torch.unique(batch_edge_index.flatten())\n",
    "                n_batch_nodes = len(unique_nodes)\n",
    "                \n",
    "                node_mapping = torch.full((x.shape[0],), -1, dtype=torch.long, device=self.device)\n",
    "                node_mapping[unique_nodes] = torch.arange(n_batch_nodes, device=self.device)\n",
    "                \n",
    "                subgraph_x = x[unique_nodes]\n",
    "                \n",
    "                node_mask = torch.zeros(x.shape[0], dtype=torch.bool, device=self.device)\n",
    "                node_mask[unique_nodes] = True\n",
    "                edge_mask = node_mask[edge_index[0]] & node_mask[edge_index[1]]\n",
    "                subgraph_edges = edge_index[:, edge_mask]\n",
    "                subgraph_edge_index = node_mapping[subgraph_edges]\n",
    "                \n",
    "                batch_edge_index_remapped = node_mapping[batch_edge_index]\n",
    "                \n",
    "                with autocast('cuda'):\n",
    "                    out = self.model(subgraph_x, subgraph_edge_index, batch_edge_index_remapped)\n",
    "                    loss = criterion(out, batch_labels)\n",
    "                \n",
    "                total_loss += loss.item() * len(batch_indices)\n",
    "                pred = out.float().max(1)[1]\n",
    "                all_preds.extend(pred.cpu().numpy())\n",
    "                all_labels.extend(batch_labels.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / n_val\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        return avg_loss, acc, f1\n",
    "    \n",
    "    def train(self, x, edge_index, edge_labels, train_mask, val_mask,\n",
    "              epochs=50, lr=0.001, weight_decay=5e-4, patience=30, batch_size=1024):\n",
    "        \"\"\"Training loop with progress tracking\"\"\"\n",
    "        \n",
    "        n_train = int(train_mask.sum())\n",
    "        n_batches = (n_train + batch_size - 1) // batch_size\n",
    "        \n",
    "        print(f\"\\nâ™» Training GAT-DDI Model\")\n",
    "        print(f\"{'='*40}\")\n",
    "        print(f\"â€¢ Device: {self.device}\")\n",
    "        print(f\"â€¢ Parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
    "        print(f\"â€¢ Batch size: {batch_size} edges\")\n",
    "        print(f\"â€¢ Batches per epoch: {n_batches}\")  # âœ… NEW: Show batches\n",
    "        print(f\"â€¢ Training edges: {n_train}\")\n",
    "        print(f\"â€¢ Validation edges: {int(val_mask.sum())}\")\n",
    "        print(f\"â€¢ Mixed Precision: ENABLED âœ…\")\n",
    "        print(f\"{'='*40}\")\n",
    "        \n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        best_val_acc = 0\n",
    "        best_val_f1 = 0\n",
    "        best_epoch = 0\n",
    "        patience_counter = 0\n",
    "        best_model_state = None\n",
    "        start_time = time.time()\n",
    "        \n",
    "        print(\"\\n\" + \"_\"*80)\n",
    "        print(\"EPOCH | TRAIN LOSS | TRAIN ACC | TRAIN F1 | VAL LOSS | VAL ACC | VAL F1 | TIME\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_start = time.time()\n",
    "            \n",
    "            # âœ… NEW: Print epoch number before training starts\n",
    "            print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "            \n",
    "            train_loss, train_acc, train_f1 = self.train_epoch(\n",
    "                x, edge_index, edge_labels, train_mask, optimizer, criterion, batch_size\n",
    "            )\n",
    "            \n",
    "            val_loss, val_acc, val_f1 = self.validate(\n",
    "                x, edge_index, edge_labels, val_mask, criterion, batch_size\n",
    "            )\n",
    "            \n",
    "            epoch_time = time.time() - epoch_start\n",
    "            \n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['val_acc'].append(val_acc)\n",
    "            self.history['train_f1'].append(train_f1)\n",
    "            self.history['val_f1'].append(val_f1)\n",
    "            \n",
    "            # Track best model\n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_acc = val_acc\n",
    "                best_val_f1 = val_f1\n",
    "                best_epoch = epoch + 1\n",
    "                patience_counter = 0\n",
    "                best_model_state = {k: v.cpu().clone() for k, v in self.model.state_dict().items()}\n",
    "                marker = \" â˜…\"\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                marker = \"\"\n",
    "            \n",
    "            # âœ… CHANGED: Print EVERY epoch (not just every 5)\n",
    "            print(f\"{epoch+1:5d} | {train_loss:10.4f} | {train_acc:9.4f} | \"\n",
    "                  f\"{train_f1:8.4f} | {val_loss:8.4f} | {val_acc:7.4f} | {val_f1:6.4f} | \"\n",
    "                  f\"{epoch_time:.1f}s{marker}\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nâŠ˜ Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "            \n",
    "            # Memory cleanup every epoch\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        print(\"-\"*80)\n",
    "        print(f\"(âœ“) Training completed in {training_time/60:.1f} minutes\")\n",
    "        print(f\"â˜… Best epoch: {best_epoch}\")\n",
    "        print(f\"â˜… Best validation accuracy: {best_val_acc:.4f}\")\n",
    "        print(f\"â˜… Best validation F1-score: {best_val_f1:.4f}\")\n",
    "        print(\"_\"*80)\n",
    "        \n",
    "        # Restore best model\n",
    "        if best_model_state is not None:\n",
    "            self.model.load_state_dict(best_model_state)\n",
    "            torch.save(best_model_state, f'models/best_GAT_DDI_{timestamp}.pth')\n",
    "            print(f\"âœ… Best model saved to models/best_GAT_DDI_{timestamp}.pth\")\n",
    "        \n",
    "        self.best_model_state = best_model_state\n",
    "        self.training_metadata = {\n",
    "            'best_epoch': best_epoch,\n",
    "            'total_epochs': epoch + 1,\n",
    "            'best_val_acc': best_val_acc,\n",
    "            'best_val_f1': best_val_f1,\n",
    "            'training_time': training_time,\n",
    "            'early_stopped': patience_counter >= patience\n",
    "        }\n",
    "        \n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation class for GAT models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: GAT_DDI, device: torch.device):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "    \n",
    "    def test(self, data):\n",
    "        \"\"\"Comprehensive testing with multiple metrics\"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_edge_index = data['edge_index'][:, data['test_mask']]\n",
    "            out = self.model(data['node_features'], data['edge_index'], test_edge_index)\n",
    "            \n",
    "            pred = out.max(1)[1]\n",
    "            y_true = data['edge_labels'][data['test_mask']].cpu().numpy()\n",
    "            y_pred = pred.cpu().numpy()\n",
    "            \n",
    "            unique_labels = np.unique(np.concatenate([y_true, y_pred]))\n",
    "            \n",
    "            # Calculate metrics\n",
    "            test_acc = accuracy_score(y_true, y_pred)\n",
    "            f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "            \n",
    "            class_report = classification_report(y_true, y_pred, \n",
    "                                                labels=unique_labels,\n",
    "                                                output_dict=True,\n",
    "                                                zero_division=0)\n",
    "            conf_matrix = confusion_matrix(y_true, y_pred, labels=unique_labels)\n",
    "            \n",
    "            return {\n",
    "                'accuracy': test_acc,\n",
    "                'f1_score': f1, \n",
    "                'classification_report': class_report,\n",
    "                'confusion_matrix': conf_matrix,\n",
    "                'predictions': y_pred,\n",
    "                'true_labels': y_true\n",
    "            }\n",
    "    \n",
    "    def visualize_embeddings(self, data, save_path=f'images/GAT_visualize_embeddings_{timestamp}.png'):\n",
    "        \"\"\"\n",
    "        Visualize node embeddings using t-SNE\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Get embeddings from first layer\n",
    "            x = self.model.dropout_layer(data.x)\n",
    "            embeddings = self.model.attention1(x, data.adj_mat)\n",
    "            embeddings = embeddings.cpu().numpy()\n",
    "            \n",
    "            # Apply t-SNE\n",
    "            print(\"ðŸ”„ Computing t-SNE embeddings...\")\n",
    "            tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "            embeddings_2d = tsne.fit_transform(embeddings)\n",
    "            \n",
    "            # Plot\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "                                c=data.y.cpu().numpy(), cmap='tab10', alpha=0.7, s=50)\n",
    "            plt.colorbar(scatter, label='Node Class')\n",
    "            plt.title('GAT Node Embeddings Visualization (t-SNE)', fontsize=16, fontweight='bold')\n",
    "            plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "            plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plotting and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, save_path=f'images/DDI_training_history_{timestamp}.png'):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(history['train_loss'], label='Train', linewidth=2)\n",
    "    axes[0].plot(history['val_loss'], label='Validation', linewidth=2)\n",
    "    axes[0].set_title('Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].plot(history['train_acc'], label='Train', linewidth=2)\n",
    "    axes[1].plot(history['val_acc'], label='Validation', linewidth=2)\n",
    "    axes[1].set_title('Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # F1-Score\n",
    "    axes[2].plot(history['train_f1'], label='Train', linewidth=2)\n",
    "    axes[2].plot(history['val_f1'], label='Validation', linewidth=2)\n",
    "    axes[2].set_title('F1-Score', fontsize=14, fontweight='bold')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('F1-Score')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"âœ“ Saved training history to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________\n",
      "LOADING DRUG-DRUG INTERACTION DATA\n",
      "----------------------------------------\n",
      "âœ“ Loaded 191808 DDI pairs\n",
      "âœ“ Loaded 1706 drug SMILES\n",
      "________________________________________\n",
      "\n",
      "_____________________________________________\n",
      "Extracting molecular features from SMILES...\n",
      "---------------------------------------------\n",
      "200/1706 drugs processed \n",
      "400/1706 drugs processed \n",
      "600/1706 drugs processed \n",
      "800/1706 drugs processed \n",
      "1000/1706 drugs processed \n",
      "1200/1706 drugs processed \n",
      "1400/1706 drugs processed \n",
      "1600/1706 drugs processed \n",
      "---------------------------------------------\n",
      "âœ“ Extracted features for 1706 drugs\n",
      "_____________________________________________\n",
      "\n",
      "________________________________________\n",
      "Building DDI graph...\n",
      "â€¢ Number of nodes (drugs): 1706\n",
      "â€¢ Number of edges: 383616\n",
      "â€¢ Number of interaction types: 86\n",
      "â€¢ Feature dimension: 1032\n",
      "\n",
      "ðŸ›ˆ Data Split:\n",
      "â€¢ Train: 268531 edges (70.0%)\n",
      "â€¢ Val: 57542 edges (15.0%)\n",
      "â€¢ Test: 57543 edges (15.0%)\n",
      "__________________________________________________\n",
      "\n",
      "â›¯ Model Configuration:\n",
      "â€¢ n_features: 1032\n",
      "â€¢ n_hidden: 128\n",
      "â€¢ n_classes: 86\n",
      "â€¢ n_heads: 4\n",
      "â€¢ dropout: 0.3\n",
      "__________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "data = prepare_ddi_data()\n",
    "\n",
    "# Remove unused adjacency matrix to save VRAM\n",
    "del data['adj_mat']\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Move to device\n",
    "for key in ['node_features', 'edge_index', 'edge_labels', \n",
    "            'train_mask', 'val_mask', 'test_mask']:\n",
    "    data[key] = data[key].to(device)\n",
    "\n",
    "# Model configuration\n",
    "config = {\n",
    "    'n_features': data['node_features'].shape[1],\n",
    "    'n_hidden': 128, # 256 OR 128\n",
    "    'n_classes': data['n_classes'],\n",
    "    'n_heads': 4, # 4 OR 8\n",
    "    'dropout': 0.3\n",
    "}\n",
    "print('_'*50)\n",
    "print(f\"\\nâ›¯ Model Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"â€¢ {key}: {value}\")\n",
    "print('_'*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â™» Training GAT-DDI Model\n",
      "========================================\n",
      "â€¢ Device: cuda\n",
      "â€¢ Parameters: 1,927,830\n",
      "â€¢ Batch size: 512 edges\n",
      "â€¢ Batches per epoch: 525\n",
      "â€¢ Training edges: 268531\n",
      "â€¢ Validation edges: 57542\n",
      "â€¢ Mixed Precision: ENABLED âœ…\n",
      "========================================\n",
      "\n",
      "________________________________________________________________________________\n",
      "EPOCH | TRAIN LOSS | TRAIN ACC | TRAIN F1 | VAL LOSS | VAL ACC | VAL F1 | TIME\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 1/30:\n",
      "    Batch 520/525 | Loss: 1.8163 | Elapsed: 413s | ETA: 4sss\n",
      "    1 |     2.2381 |    0.3993 |   0.2902 |   1.7856 |  0.4395 | 0.3031 | 420.5s â˜…\n",
      "Epoch 2/30:\n",
      "    Batch 520/525 | Loss: 1.7222 | Elapsed: 307s | ETA: 3sss\n",
      "    2 |     1.8086 |    0.4541 |   0.3447 |   1.7167 |  0.5049 | 0.3943 | 312.5s â˜…\n",
      "Epoch 3/30:\n",
      "    Batch 400/525 | Loss: 1.5490 | Elapsed: 328s | ETA: 102s"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = GAT_DDI(**config)\n",
    "trainer = DDITrainer(model, device)\n",
    "\n",
    "# âœ… Train with smaller batch size for 4GB GPU\n",
    "history = trainer.train(\n",
    "    data['node_features'], \n",
    "    data['edge_index'],\n",
    "    data['edge_labels'], \n",
    "    data['train_mask'], \n",
    "    data['val_mask'],\n",
    "    epochs=30, \n",
    "    lr=0.001, \n",
    "    weight_decay=5e-4, \n",
    "    patience=30,\n",
    "    batch_size=512  # 800 OR 512 OR 256\n",
    ")\n",
    "\n",
    "# Plot history\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate\n",
    "loaded_model = model.load_state_dict(torch.load(f'models/best_GAT_DDI_{timestamp}.pth'))\n",
    "# results = evaluate_model(model, data, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "saved_model = torch.save({\n",
    "    'config': config,\n",
    "    'history': history,\n",
    "    'results': results,\n",
    "    'timestamp': timestamp\n",
    "}, f'models/GAT_DDI_results_{timestamp}.pth')\n",
    "\n",
    "# save_file(model.state_dict(), f'models/best_GAT_DDI_{timestamp}.safetensors')\n",
    "\n",
    "print(f\"\\nâœ… All results saved!\")\n",
    "print(f\"â€¢ Model: models/best_GAT_DDI_{timestamp}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Model\n",
    "# Load best model and evaluate\n",
    "\n",
    "model.load_state_dict(torch.load(f'models/best_GAT_DDI_{timestamp}.pth'))\n",
    "\n",
    "evaluator = ModelEvaluator(model, device)\n",
    "\n",
    "test_results = evaluator.test(data)\n",
    "\n",
    "print(f\"ðŸŽ¯ Test Result Accuracy: {test_results['accuracy']:.4f}\")\n",
    "print(\"\\nðŸ“Š Classification Report:\")\n",
    "print(classification_report(test_results['true_labels'], test_results['predictions']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Saving Logs and Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = evaluator.test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare the new log entry with rounded values\n",
    "current_log = {\n",
    "    \"date\": timestamp,\n",
    "    \"graph_details\": {\n",
    "        \"num_nodes\": data['node_features'].shape[0],  # âœ… ADD: Number of drugs\n",
    "        \"num_edges\": data['edge_index'].shape[1],  # âœ… FIX: Total edges (not just first row)\n",
    "        \"num_interaction_types\": data['n_classes'],\n",
    "        \"feature_dimension\": data['node_features'].shape[1]\n",
    "    },\n",
    "    \"config\": config,\n",
    "    \"runtime_info\": {\n",
    "        \"device\": str(device),\n",
    "        \"total_parameters\": sum(p.numel() for p in model.parameters()),\n",
    "        \"trainable_parameters\": sum(p.numel() for p in model.parameters() if p.requires_grad),  # âœ… ADD\n",
    "        \"model_size_mb\": round(sum(p.numel() for p in model.parameters()) * 4 / 1024 / 1024, 2),  # âœ… ADD\n",
    "        \"training_edges\": int(data['train_mask'].sum()),\n",
    "        \"validation_edges\": int(data['val_mask'].sum()),\n",
    "        \"test_edges\": int(data['test_mask'].sum())  # âœ… ADD: Test set size\n",
    "    },\n",
    "    \"training_info\": {  # âœ… ADD: Training details\n",
    "        \"epochs_completed\": len(history['train_loss']),\n",
    "        \"best_epoch\": history['val_f1'].index(max(history['val_f1'])) + 1,\n",
    "        \"learning_rate\": 0.001,  # From your training config\n",
    "        \"batch_size\": 7000,\n",
    "        \"weight_decay\": 5e-4\n",
    "    },\n",
    "    \"metrics\": {\n",
    "        # Training metrics (rounded to 3 decimals)\n",
    "        \"train_loss\": round(history['train_loss'][-1], 3),  # âœ… ROUNDED\n",
    "        \"train_acc\": round(history['train_acc'][-1], 3),    # âœ… ROUNDED\n",
    "        \"train_f1\": round(history['train_f1'][-1], 3),      # âœ… ROUNDED\n",
    "        \n",
    "        # Validation metrics (rounded to 3 decimals)\n",
    "        \"val_loss\": round(history['val_loss'][-1], 3),      # âœ… ROUNDED\n",
    "        \"val_acc\": round(history['val_acc'][-1], 3),        # âœ… ROUNDED\n",
    "        \"val_f1\": round(history['val_f1'][-1], 3),          # âœ… ROUNDED\n",
    "        \n",
    "        # Best validation metrics\n",
    "        \"best_val_acc\": round(max(history['val_acc']), 3),  # âœ… ADD\n",
    "        \"best_val_f1\": round(max(history['val_f1']), 3),    # âœ… ADD\n",
    "        \n",
    "        # Test metrics (if available)\n",
    "        \"test_acc\": round(results['accuracy'], 3) if 'results' in locals() else None,  # âœ… ADD\n",
    "        \"test_f1\": round(results['f1_score'], 3) if 'results' in locals() and 'f1_score' in results else None  # âœ… ADD\n",
    "    }\n",
    "}\n",
    "\n",
    "# 2. File handling: Load existing data or create a new list\n",
    "log_file = 'model_evals_log.json'\n",
    "\n",
    "if os.path.exists(log_file):\n",
    "    with open(log_file, 'r') as f:\n",
    "        try:\n",
    "            logs_list = json.load(f)\n",
    "            if not isinstance(logs_list, list):\n",
    "                logs_list = []\n",
    "        except json.JSONDecodeError:\n",
    "            logs_list = []\n",
    "else:\n",
    "    logs_list = []\n",
    "\n",
    "# 3. Insert new log at the TOP (index 0)\n",
    "logs_list.insert(0, current_log)\n",
    "\n",
    "# 4. Save back to file with nice formatting\n",
    "with open(log_file, 'w') as f:\n",
    "    json.dump(logs_list, f, indent=4)\n",
    "\n",
    "print(f\"(âœ“) Log successfully saved to {log_file}!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_top_classes(test_results, top_n=10, save_path=f'images/GAT_confusion_matrix_{timestamp}.png'):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix for top N most frequent classes only\n",
    "    \"\"\"\n",
    "    y_true = test_results['true_labels']\n",
    "    y_pred = test_results['predictions']\n",
    "    \n",
    "    # Get top N most frequent classes in test set\n",
    "    unique, counts = np.unique(y_true, return_counts=True)\n",
    "    top_classes = unique[np.argsort(counts)[-top_n:]][::-1]  # Top N by frequency\n",
    "    \n",
    "    # Filter predictions to only include top classes\n",
    "    mask = np.isin(y_true, top_classes)\n",
    "    y_true_filtered = y_true[mask]\n",
    "    y_pred_filtered = y_pred[mask]\n",
    "    \n",
    "    # Create confusion matrix for top classes only\n",
    "    conf_matrix = confusion_matrix(y_true_filtered, y_pred_filtered, labels=top_classes)\n",
    "    \n",
    "    # Create class names\n",
    "    class_names = [f'Type {i}' for i in top_classes]\n",
    "    \n",
    "    # Plot with larger figure\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Count'}, linewidths=0.5)\n",
    "    \n",
    "    plt.title(f'Confusion Matrix - Top {top_n} Interaction Types', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Predicted Class', fontsize=12)\n",
    "    plt.ylabel('True Class', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"âœ“ Saved confusion matrix for top {top_n} classes to {save_path}\")\n",
    "    print(f\"  Top classes: {top_classes}\")\n",
    "\n",
    "# Plot top 10 most frequent interaction types\n",
    "plot_confusion_matrix_top_classes(test_results, top_n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_normalized_confusion_matrix(test_results, top_n=15, save_path=f'images/GAT_confusion_matrix_normalized_{timestamp}.png'):\n",
    "    \"\"\"\n",
    "    Plot normalized confusion matrix (percentages) for top N classes\n",
    "    \"\"\"\n",
    "    y_true = test_results['true_labels']\n",
    "    y_pred = test_results['predictions']\n",
    "    \n",
    "    # Get top N classes\n",
    "    unique, counts = np.unique(y_true, return_counts=True)\n",
    "    top_classes = unique[np.argsort(counts)[-top_n:]][::-1]\n",
    "    \n",
    "    # Filter\n",
    "    mask = np.isin(y_true, top_classes)\n",
    "    y_true_filtered = y_true[mask]\n",
    "    y_pred_filtered = y_pred[mask]\n",
    "    \n",
    "    # Normalized confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true_filtered, y_pred_filtered, labels=top_classes)\n",
    "    conf_matrix_norm = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    class_names = [f'Type {i}' for i in top_classes]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    sns.heatmap(conf_matrix_norm, annot=True, fmt='.2f', cmap='RdYlGn',\n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Proportion'}, linewidths=0.5,\n",
    "                vmin=0, vmax=1)\n",
    "    \n",
    "    plt.title(f'Normalized Confusion Matrix - Top {top_n} Types', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Predicted Class', fontsize=12)\n",
    "    plt.ylabel('True Class', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"âœ“ Saved normalized confusion matrix to {save_path}\")\n",
    "\n",
    "# Plot normalized matrix\n",
    "plot_normalized_confusion_matrix(test_results, top_n=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_summary(test_results, save_path=f'images/GAT_confusion_summary_{timestamp}.png'):\n",
    "    \"\"\"\n",
    "    Plot summary statistics instead of full confusion matrix\n",
    "    \"\"\"\n",
    "    y_true = test_results['true_labels']\n",
    "    y_pred = test_results['predictions']\n",
    "    \n",
    "    # Calculate per-class accuracy\n",
    "    unique_classes = np.unique(y_true)\n",
    "    class_accuracies = []\n",
    "    class_counts = []\n",
    "    \n",
    "    for cls in unique_classes:\n",
    "        mask = y_true == cls\n",
    "        if mask.sum() > 0:\n",
    "            acc = (y_pred[mask] == cls).sum() / mask.sum()\n",
    "            class_accuracies.append(acc)\n",
    "            class_counts.append(mask.sum())\n",
    "        else:\n",
    "            class_accuracies.append(0)\n",
    "            class_counts.append(0)\n",
    "    \n",
    "    # Sort by frequency\n",
    "    sorted_indices = np.argsort(class_counts)[::-1][:20]  # Top 20\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    ax1.barh([f'Type {unique_classes[i]}' for i in sorted_indices],\n",
    "             [class_accuracies[i] for i in sorted_indices],\n",
    "             color='steelblue')\n",
    "    ax1.set_xlabel('Accuracy', fontsize=12)\n",
    "    ax1.set_title('Per-Class Accuracy (Top 20)', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Class distribution\n",
    "    ax2.barh([f'Type {unique_classes[i]}' for i in sorted_indices],\n",
    "             [class_counts[i] for i in sorted_indices],\n",
    "             color='coral')\n",
    "    ax2.set_xlabel('Sample Count', fontsize=12)\n",
    "    ax2.set_title('Class Distribution (Top 20)', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"âœ“ Saved confusion summary to {save_path}\")\n",
    "\n",
    "# Plot summary\n",
    "plot_confusion_summary(test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix - only classes present in test set\n",
    "unique_labels = np.unique(test_results['true_labels'])\n",
    "class_names = [f'Type {i}' for i in unique_labels]\n",
    "plot_confusion_matrix(test_results['confusion_matrix'], class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model performance\n",
    "def analyze_results(history, test_results, data, model):\n",
    "    \"\"\"\n",
    "    Provide comprehensive analysis of model performance\n",
    "    \"\"\"\n",
    "    print(\"ðŸ“ˆ MODEL PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Training metrics\n",
    "    print(f\"\\nTraining Metrics:\")\n",
    "    print(f\"â€¢ Final Training Accuracy: {history['train_acc'][-1]:.4f}\")\n",
    "    print(f\"â€¢ Final Dev / Validation Accuracy: {history['val_acc'][-1]:.4f}\")\n",
    "    print(f\"â€¢ Best Validation Accuracy: {max(history['val_acc']):.4f}\")\n",
    "    print(f\"â€¢ Final Training F1: {history['train_f1'][-1]:.4f}\")\n",
    "    print(f\"â€¢ Final Validation F1: {history['val_f1'][-1]:.4f}\")\n",
    "    print(f\"â€¢ Best Validation F1: {max(history['val_f1']):.4f}\")\n",
    "    print(f\"â€¢ Total Epochs: {len(history['train_loss'])}\")\n",
    "    \n",
    "    # Test metrics\n",
    "    print(f\"\\nTest Metrics:\")\n",
    "    print(f\"â€¢ Test Accuracy: {test_results['accuracy']:.4f}\")\n",
    "    if 'f1_score' in test_results:\n",
    "        print(f\"â€¢ Test F1-Score: {test_results['f1_score']:.4f}\")\n",
    "    \n",
    "    # Per-class performance (only for classes in test set)\n",
    "    print(f\"\\nðŸ“Š Per-Class Performance:\")\n",
    "    unique_labels = np.unique(test_results['true_labels'])\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        label_str = str(label)\n",
    "        if label_str in test_results['classification_report']:\n",
    "            report = test_results['classification_report'][label_str]\n",
    "            precision = report['precision']\n",
    "            recall = report['recall']\n",
    "            f1 = report['f1-score']\n",
    "            print(f\"   â€¢ Type {label}: Precision={precision:.3f}, Recall={recall:.3f}, F1={f1:.3f}\")\n",
    "    \n",
    "    # Model complexity\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\nðŸ”§ Model Complexity:\")\n",
    "    print(f\"   â€¢ Total Parameters: {total_params:,}\")\n",
    "    print(f\"   â€¢ Trainable Parameters: {trainable_params:,}\")\n",
    "    print(f\"   â€¢ Model Size: ~{total_params * 4 / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    # Dataset info\n",
    "    print(f\"\\nðŸ“Š Dataset Info:\")\n",
    "    print(f\"   â€¢ Total Drugs (Nodes): {data['node_features'].shape[0]}\")\n",
    "    print(f\"   â€¢ Total Interactions (Edges): {data['edge_index'].shape[1]}\")\n",
    "    print(f\"   â€¢ Feature Dimension: {data['node_features'].shape[1]}\")\n",
    "    print(f\"   â€¢ Number of Interaction Types: {data['n_classes']}\")\n",
    "    print(f\"   â€¢ Classes in Test Set: {len(unique_labels)}\")\n",
    "\n",
    "# Run analysis\n",
    "analyze_results(history, test_results, data, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
