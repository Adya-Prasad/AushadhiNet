{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Attention Networks (GAT) - Complete Professional Implementation\n",
    "\n",
    "This notebook provides a comprehensive implementation of Graph Attention Networks (GAT) based on the paper \"Graph Attention Networks\" by VeliÄkoviÄ‡ et al.\n",
    "\n",
    "### Key Features:\n",
    "- âœ… Multi-head attention mechanism\n",
    "- âœ… Professional training pipeline with early stopping\n",
    "- âœ… Comprehensive evaluation metrics\n",
    "- âœ… Visualization capabilities\n",
    "- âœ… Error handling and logging\n",
    "\n",
    "### Architecture Overview:\n",
    "GATs work on graph data where nodes represent entities and edges represent relationships. The attention mechanism allows nodes to focus on the most relevant neighbors, similar to transformers but adapted for graph structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from safetensors.torch import save_file, load_file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator, Descriptors\n",
    "import networkx as nx\n",
    "from datetime import datetime\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "# Memory optimization settings for GPU, Aggressive memory management\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "    \n",
    "    # Force garbage collection\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(\"Memory optimization enabled âœ“\")\n",
    "\n",
    "# specific style date time saving\n",
    "timestamp = datetime.now().strftime(\"%d_%b_%H-%M\")\n",
    "\n",
    "# validating required directories:\n",
    "required_directories = ['images', 'models', 'results']\n",
    "for folder in required_directories:\n",
    "    if not os.path.exists(folder):\n",
    "        print(f\"Directory `{folder}/` not found âœ˜ Creating...\")\n",
    "        os.makedirs(folder)\n",
    "    else:\n",
    "        print(f\"Directory `{folder}/` exists âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DDI data\n",
    "print(\"=\"*60)\n",
    "print(\"EXPLORING DRUG-DRUG INTERACTION DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load DDI interactions\n",
    "ddi_df = pd.read_csv('dataset/drugdata/ddis.csv')\n",
    "print(f\"\\nðŸ“Š DDI Dataset Shape: {ddi_df.shape}\")\n",
    "print(f\"Columns: {ddi_df.columns.tolist()}\")\n",
    "print(f\"\\nðŸ”¬ Interaction Types Distribution:\")\n",
    "print(ddi_df['type'].value_counts())\n",
    "print(f\"\\nðŸ“‹ Sample DDI Data:\")\n",
    "print(ddi_df.head(10))\n",
    "\n",
    "# Load drug SMILES\n",
    "smiles_df = pd.read_csv('dataset/drugdata/drug_smiles.csv')\n",
    "print(f\"\\nðŸ’Š Drug SMILES Dataset Shape: {smiles_df.shape}\")\n",
    "print(f\"Columns: {smiles_df.columns.tolist()}\")\n",
    "print(f\"\\nðŸ“‹ Sample SMILES Data:\")\n",
    "print(smiles_df.head(10))\n",
    "\n",
    "# Get unique drugs\n",
    "unique_drugs_ddi = set(ddi_df['d1'].unique()) | set(ddi_df['d2'].unique())\n",
    "print(f\"\\nðŸ“ˆ Statistics:\")\n",
    "print(f\"â€¢ Total DDI pairs: {len(ddi_df)}\")\n",
    "print(f\"â€¢ Unique drugs in DDI: {len(unique_drugs_ddi)}\")\n",
    "print(f\"â€¢ Drugs with SMILES: {len(smiles_df)}\")\n",
    "print(f\"â€¢ Interaction type 0: {(ddi_df['type'] == 0).sum()}\")\n",
    "print(f\"â€¢ Interaction type 1: {(ddi_df['type'] == 1).sum()}\")\n",
    "\n",
    "# Check overlap\n",
    "drugs_with_smiles = set(smiles_df['drug_id'].unique())\n",
    "overlap = unique_drugs_ddi & drugs_with_smiles\n",
    "print(f\"â€¢ Drugs with both DDI and SMILES: {len(overlap)}\")\n",
    "print(f\"â€¢ Coverage: {len(overlap)/len(unique_drugs_ddi)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ddi_data():\n",
    "    \"\"\"\n",
    "    Load and prepare DDI data\n",
    "    \"\"\"\n",
    "    print(\"_\"*40)\n",
    "    print(\"LOADING DRUG-DRUG INTERACTION DATA\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Load data\n",
    "    ddi_df = pd.read_csv('dataset/drugdata/ddis.csv')\n",
    "    smiles_df = pd.read_csv('dataset/drugdata/drug_smiles.csv')\n",
    "    \n",
    "    print(f\"âœ“ Loaded {len(ddi_df)} DDI pairs\")\n",
    "    print(f\"âœ“ Loaded {len(smiles_df)} drug SMILES\")\n",
    "    print(\"_\"*40)\n",
    "    \n",
    "    # Extract features\n",
    "    feature_extractor = DrugFeatureExtractor()\n",
    "    drug_features = feature_extractor.extract_all_features(smiles_df)\n",
    "    \n",
    "    # Build graph\n",
    "    graph_builder = DDIGraphBuilder(ddi_df, drug_features)\n",
    "    node_features, edge_index, edge_labels, n_classes = graph_builder.build_graph()\n",
    "    \n",
    "    # Create adjacency matrix\n",
    "    n_nodes = node_features.shape[0]\n",
    "    adj_mat = torch.zeros(n_nodes, n_nodes, 1)\n",
    "    adj_mat[edge_index[0], edge_index[1], 0] = 1\n",
    "    \n",
    "    # Add self-loops\n",
    "    adj_mat[range(n_nodes), range(n_nodes), 0] = 1\n",
    "    \n",
    "    # Split data\n",
    "    n_edges = edge_index.shape[1]\n",
    "    indices = np.arange(n_edges)\n",
    "    \n",
    "    train_idx, temp_idx = train_test_split(indices, test_size=0.3, random_state=42)\n",
    "    val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n",
    "    \n",
    "    train_mask = torch.zeros(n_edges, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(n_edges, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(n_edges, dtype=torch.bool)\n",
    "    \n",
    "    train_mask[train_idx] = True\n",
    "    val_mask[val_idx] = True\n",
    "    test_mask[test_idx] = True\n",
    "    \n",
    "    print(f\"\\nðŸ›ˆ Data Split:\")\n",
    "    print(f\"â€¢ Train: {train_mask.sum()} edges ({train_mask.sum()/n_edges*100:.1f}%)\")\n",
    "    print(f\"â€¢ Val: {val_mask.sum()} edges ({val_mask.sum()/n_edges*100:.1f}%)\")\n",
    "    print(f\"â€¢ Test: {test_mask.sum()} edges ({test_mask.sum()/n_edges*100:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'node_features': node_features,\n",
    "        'edge_index': edge_index,\n",
    "        'edge_labels': edge_labels,\n",
    "        'adj_mat': adj_mat,\n",
    "        'train_mask': train_mask,\n",
    "        'val_mask': val_mask,\n",
    "        'test_mask': test_mask,\n",
    "        'n_classes': n_classes,\n",
    "        'label_encoder': graph_builder.label_encoder\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature and Pattern Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrugFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extract molecular features from SMILES strings using RDKit\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def smiles_to_features(self, smiles):\n",
    "        \"\"\"\n",
    "        Convert SMILES to molecular fingerprint and descriptors\n",
    "        \n",
    "        Args:\n",
    "            smiles: SMILES string\n",
    "            \n",
    "        Returns:\n",
    "            Feature vector\n",
    "        \"\"\"\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                return None\n",
    "            \n",
    "            # Morgan fingerprint\n",
    "            mfpgen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=1024)\n",
    "            fp = fp = mfpgen.GetFingerprint(mol)\n",
    "            fp_array = np.array(fp)\n",
    "            \n",
    "            # Molecular descriptors\n",
    "            descriptors = [\n",
    "                Descriptors.MolWt(mol),\n",
    "                Descriptors.MolLogP(mol),\n",
    "                Descriptors.NumHDonors(mol),\n",
    "                Descriptors.NumHAcceptors(mol),\n",
    "                Descriptors.TPSA(mol),\n",
    "                Descriptors.NumRotatableBonds(mol),\n",
    "                Descriptors.NumAromaticRings(mol),\n",
    "                Descriptors.FractionCSP3(mol),\n",
    "            ]\n",
    "            \n",
    "            # Combine fingerprint and descriptors\n",
    "            features = np.concatenate([fp_array, descriptors])\n",
    "            \n",
    "            return features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing SMILES: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_all_features(self, smiles_df):\n",
    "        \"\"\"\n",
    "        Extract features for all drugs\n",
    "        \n",
    "        Args:\n",
    "            smiles_df: DataFrame with drug_id and smiles columns\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping drug_id to feature vector\n",
    "        \"\"\"\n",
    "        print(\"\\n\"+\"_\"*45)\n",
    "        print(\"Extracting molecular features from SMILES...\")\n",
    "        print(\"-\"*45)\n",
    "\n",
    "        drug_features = {}\n",
    "        failed = 0\n",
    "        \n",
    "        for idx, row in smiles_df.iterrows():\n",
    "            drug_id = row['drug_id']\n",
    "            smiles = row['smiles']\n",
    "            \n",
    "            features = self.smiles_to_features(smiles)\n",
    "            \n",
    "            if features is not None:\n",
    "                drug_features[drug_id] = features\n",
    "            else:\n",
    "                failed += 1\n",
    "            \n",
    "            if (idx + 1) % 200 == 0:\n",
    "                print(f\"{idx + 1}/{len(smiles_df)} drugs processed \")\n",
    "        print(\"-\"*45)\n",
    "        print(f\"âœ“ Extracted features for {len(drug_features)} drugs\")\n",
    "        if failed > 0:\n",
    "            print(f\"âš  Failed to process {failed} drugs\")\n",
    "        print(\"_\"*45 + \"\\n\")\n",
    "        return drug_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GAT Architecture Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDIGraphBuilder:\n",
    "    \"\"\"\n",
    "    Build graph structure from DDI data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ddi_df, drug_features):\n",
    "        self.ddi_df = ddi_df\n",
    "        self.drug_features = drug_features\n",
    "        self.drug_to_idx = {}\n",
    "        self.idx_to_drug = {}\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def build_graph(self):\n",
    "        \"\"\"\n",
    "        Build graph from DDI data\n",
    "        \n",
    "        Returns:\n",
    "            Node features, edge indices, edge labels\n",
    "        \"\"\"\n",
    "        print(\"_\"*40)\n",
    "        print(\"Building DDI graph...\")\n",
    "        \n",
    "        # Get unique drugs\n",
    "        unique_drugs = sorted(list(self.drug_features.keys()))\n",
    "        self.drug_to_idx = {drug: idx for idx, drug in enumerate(unique_drugs)}\n",
    "        self.idx_to_drug = {idx: drug for drug, idx in self.drug_to_idx.items()}\n",
    "        \n",
    "        print(f\"â€¢ Number of nodes (drugs): {len(unique_drugs)}\")\n",
    "        \n",
    "        # Create node feature matrix\n",
    "        feature_dim = len(list(self.drug_features.values())[0])\n",
    "        node_features = np.zeros((len(unique_drugs), feature_dim))\n",
    "        \n",
    "        for drug, idx in self.drug_to_idx.items():\n",
    "            node_features[idx] = self.drug_features[drug]\n",
    "        \n",
    "        # Normalize features\n",
    "        node_features = (node_features - node_features.mean(axis=0)) / (node_features.std(axis=0) + 1e-8)\n",
    "        \n",
    "        # Build edges from DDI data\n",
    "        edge_list = []\n",
    "        edge_labels = []\n",
    "        \n",
    "        for _, row in self.ddi_df.iterrows():\n",
    "            d1, d2, interaction_type = row['d1'], row['d2'], row['type']\n",
    "            \n",
    "            if d1 in self.drug_to_idx and d2 in self.drug_to_idx:\n",
    "                idx1 = self.drug_to_idx[d1]\n",
    "                idx2 = self.drug_to_idx[d2]\n",
    "                \n",
    "                # Add bidirectional edges\n",
    "                edge_list.append([idx1, idx2])\n",
    "                edge_list.append([idx2, idx1])\n",
    "                edge_labels.append(interaction_type)\n",
    "                edge_labels.append(interaction_type)\n",
    "        \n",
    "        edge_index = torch.tensor(edge_list, dtype=torch.long).t()\n",
    "        \n",
    "        # Encode labels\n",
    "        edge_labels = self.label_encoder.fit_transform(edge_labels)\n",
    "        edge_labels = torch.tensor(edge_labels, dtype=torch.long)\n",
    "        \n",
    "        print(f\"â€¢ Number of edges: {len(edge_list)}\")\n",
    "        print(f\"â€¢ Number of interaction types: {len(self.label_encoder.classes_)}\")\n",
    "        print(f\"â€¢ Feature dimension: {feature_dim}\")\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(node_features),\n",
    "            edge_index,\n",
    "            edge_labels,\n",
    "            len(self.label_encoder.classes_)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Graph Attention Layer Implementation\n",
    "    \n",
    "    Args:\n",
    "        in_features: Number of input features per node (F)\n",
    "        out_features: Number of output features per node (F')\n",
    "        n_heads: Number of attention heads (K)\n",
    "        is_concat: Whether to concatenate or average multi-head results\n",
    "        dropout: Dropout probability for regularization\n",
    "        leaky_relu_negative_slope: Negative slope for LeakyReLU activation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int, n_heads: int=8, \n",
    "                 is_concat: bool = True, dropout: float = 0.6, \n",
    "                 leaky_relu_negative_slope: float = 0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.is_concat = is_concat\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        # Calculate dimensions per head\n",
    "        if is_concat:\n",
    "            assert out_features % n_heads == 0, \"out_features must be divisible by n_heads when concatenating\"\n",
    "            self.n_hidden = out_features // n_heads\n",
    "        else:\n",
    "            self.n_hidden = out_features\n",
    "        \n",
    "        # Linear transformation: W in the paper\n",
    "        self.linear = nn.Linear(in_features, self.n_hidden * n_heads, bias=False)\n",
    "        \n",
    "        # Attention parameters (a^T) (one per head)\n",
    "        self.attn_src = nn.Parameter(torch.Tensor(1, n_heads, self.n_hidden))\n",
    "        self.attn_dst = nn.Parameter(torch.Tensor(1, n_heads, self.n_hidden))\n",
    "        \n",
    "        # Activation and normalization\n",
    "        self.activation = nn.LeakyReLU(negative_slope=leaky_relu_negative_slope)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize weights using Xavier uniform\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize layer weights using Xavier uniform initialization\"\"\"\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        nn.init.xavier_uniform_(self.attn_src)\n",
    "        nn.init.xavier_uniform_(self.attn_dst)\n",
    "    \n",
    "    def forward(self, h: torch.Tensor, adj_mat: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the attention layer\n",
    "        \n",
    "        Args:\n",
    "            h: Node embeddings [n_nodes, in_features]\n",
    "            adj_mat: Adjacency matrix [n_nodes, n_nodes, n_heads]\n",
    "            \n",
    "        Returns:\n",
    "            Updated node embeddings [n_nodes, out_features]\n",
    "        \"\"\"\n",
    "        n_nodes = h.shape[0]\n",
    "        device = h.device\n",
    "        \n",
    "        # Step 1: Linear transformation g_i = W * h_i for each head\n",
    "        g = self.linear(h).view(n_nodes, self.n_heads, self.n_hidden)\n",
    "\n",
    "        # Extract edge indices from adjacency matrix (ony non-zero entries)\n",
    "        adj_2d = adj_mat.squeeze(-1)\n",
    "        edge_index = adj_2d.nonzero(as_tuple=False).t()\n",
    "\n",
    "        if edge_index.shape[1] == 0:\n",
    "            # No edges case\n",
    "            if self.is_concat:\n",
    "                return torch.zeros(n_nodes, self.n_heads * self.n_hidden, device=device)\n",
    "            else:\n",
    "                return torch.zeros(n_nodes, self.n_hidden, device=device)\n",
    "        \n",
    "        src_nodes = edge_index[0]\n",
    "        dst_nodes = edge_index[1]\n",
    "        \n",
    "        # Get features for source and destination nodes\n",
    "        g_src = g[src_nodes]  # [n_edges, n_heads, n_hidden]\n",
    "        g_dst = g[dst_nodes]  # [n_edges, n_heads, n_hidden]\n",
    "        \n",
    "        # Compute attention scores (edge-wise, memory efficient)\n",
    "        e_src = (g_src * self.attn_src).sum(dim=-1)  # [n_edges, n_heads]\n",
    "        e_dst = (g_dst * self.attn_dst).sum(dim=-1)  # [n_edges, n_heads]\n",
    "        e = self.activation(e_src + e_dst)  # [n_edges, n_heads]\n",
    "        \n",
    "        # Softmax normalization per destination node\n",
    "        # Group by destination node for proper normalization\n",
    "        alpha = torch.zeros_like(e)\n",
    "        # Process each head separately to save memory\n",
    "        for head in range(self.n_heads):\n",
    "            e_head = e[:, head]\n",
    "            \n",
    "            # Use scatter operations instead of loops\n",
    "            # Create index for scatter\n",
    "            max_dst = dst_nodes.max().item() + 1\n",
    "            \n",
    "            # Compute max for numerical stability\n",
    "            max_vals = torch.full((max_dst,), float('-inf'), device=device)\n",
    "            max_vals.scatter_reduce_(0, dst_nodes, e_head, reduce='amax', include_self=False)\n",
    "            \n",
    "            # Subtract max and exp\n",
    "            e_stable = e_head - max_vals[dst_nodes]\n",
    "            exp_e = torch.exp(e_stable)\n",
    "            \n",
    "            # Sum exp values per destination\n",
    "            sum_exp = torch.zeros(max_dst, device=device)\n",
    "            sum_exp.scatter_add_(0, dst_nodes, exp_e)\n",
    "            \n",
    "            # Normalize\n",
    "            alpha[:, head] = exp_e / (sum_exp[dst_nodes] + 1e-16)\n",
    "        \n",
    "        # Aggregate features using attention weights\n",
    "        out = torch.zeros(n_nodes, self.n_heads, self.n_hidden, device=device)\n",
    "        \n",
    "        # Efficient aggregation using scatter_add\n",
    "        for head in range(self.n_heads):\n",
    "            # Weighted features for this head\n",
    "            weighted_features = g_src[:, head, :] * alpha[:, head].unsqueeze(-1)\n",
    "            \n",
    "            # Aggregate to destination nodes\n",
    "            out[:, head, :].scatter_add_(0, \n",
    "                                         dst_nodes.unsqueeze(-1).expand(-1, self.n_hidden),\n",
    "                                         weighted_features)\n",
    "        \n",
    "        # Concatenate or average heads\n",
    "        if self.is_concat:\n",
    "            return out.reshape(n_nodes, self.n_heads * self.n_hidden)\n",
    "        else:\n",
    "            return out.mean(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT_DDI(nn.Module):\n",
    "    \"\"\"\n",
    "    Memory-efficient GAT model with gradient checkpointing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_features, n_hidden, n_classes, n_heads=8, dropout=0.6):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # GAT layers\n",
    "        self.gat1 = GraphAttentionLayer(n_features, n_hidden, n_heads, True, dropout)\n",
    "        self.gat2 = GraphAttentionLayer(n_hidden, n_hidden, n_heads, True, dropout)\n",
    "        self.gat3 = GraphAttentionLayer(n_hidden, n_hidden // 2, n_heads, True, dropout)\n",
    "        \n",
    "        # Edge prediction layers\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_hidden // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(n_hidden // 2, n_hidden // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(n_hidden // 4, n_classes)\n",
    "        )\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        \n",
    "        # Enable gradient checkpointing for memory efficiency\n",
    "        self.use_checkpointing = True\n",
    "    \n",
    "    def _gat_forward(self, x, adj_mat, gat_layer):\n",
    "        \"\"\"Helper for gradient checkpointing\"\"\"\n",
    "        return gat_layer(x, adj_mat)\n",
    "    \n",
    "    def forward(self, x, adj_mat, edge_index):\n",
    "        \"\"\"\n",
    "        Forward pass with gradient checkpointing\n",
    "        \"\"\"\n",
    "        # Layer 1\n",
    "        x = self.dropout_layer(x)\n",
    "        if self.use_checkpointing and self.training:\n",
    "            x = torch.utils.checkpoint.checkpoint(self._gat_forward, x, adj_mat, self.gat1, use_reentrant=False)\n",
    "        else:\n",
    "            x = self.gat1(x, adj_mat)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.dropout_layer(x)\n",
    "        if self.use_checkpointing and self.training:\n",
    "            x = torch.utils.checkpoint.checkpoint(self._gat_forward, x, adj_mat, self.gat2, use_reentrant=False)\n",
    "        else:\n",
    "            x = self.gat2(x, adj_mat)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        # Layer 3\n",
    "        x = self.dropout_layer(x)\n",
    "        if self.use_checkpointing and self.training:\n",
    "            x = torch.utils.checkpoint.checkpoint(self._gat_forward, x, adj_mat, self.gat3, use_reentrant=False)\n",
    "        else:\n",
    "            x = self.gat3(x, adj_mat)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        # Edge prediction\n",
    "        src_nodes = edge_index[0]\n",
    "        dst_nodes = edge_index[1]\n",
    "        \n",
    "        edge_features = torch.cat([x[src_nodes], x[dst_nodes]], dim=1)\n",
    "        edge_pred = self.edge_mlp(edge_features)\n",
    "        \n",
    "        return edge_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Pipeline\n",
    "\n",
    "Our training pipeline includes:\n",
    "- Early stopping to prevent overfitting\n",
    "- Learning rate scheduling\n",
    "- Comprehensive logging\n",
    "- Model checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDITrainer:\n",
    "    \"\"\"\n",
    "    Professional trainer for DDI prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, device):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.history = {\n",
    "            'train_loss': [], 'val_loss': [],\n",
    "            'train_acc': [], 'val_acc': [],\n",
    "            'train_f1': [], 'val_f1': []\n",
    "        }\n",
    "    \n",
    "    def train_epoch(self, x, adj_mat, edge_index, edge_labels, train_mask, optimizer, criterion):\n",
    "        \"\"\"Train for one epoch with mini-batch processing\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        # Get training edges\n",
    "        train_indices = torch.where(train_mask)[0]\n",
    "        n_train = len(train_indices)\n",
    "        \n",
    "        # Process in mini-batches to save memory\n",
    "        batch_size = 7000  # Adjust based on GPU memory\n",
    "        n_batches = (n_train + batch_size - 1) // batch_size\n",
    "        \n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        # Shuffle training indices\n",
    "        perm = torch.randperm(n_train, device=self.device)\n",
    "        train_indices = train_indices[perm]\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, n_train)\n",
    "            batch_indices = train_indices[start_idx:end_idx]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Create batch edge index\n",
    "            batch_edge_index = edge_index[:, batch_indices]\n",
    "            batch_labels = edge_labels[batch_indices]\n",
    "            \n",
    "            # Forward pass\n",
    "            out = self.model(x, adj_mat, batch_edge_index)\n",
    "            loss = criterion(out, batch_labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            total_loss += loss.item() * len(batch_indices)\n",
    "            pred = out.max(1)[1]\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "            \n",
    "            # Clear cache periodically\n",
    "            if (i + 1) % 5 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_loss = total_loss / n_train\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        return avg_loss, acc, f1\n",
    "\n",
    "    \n",
    "    def validate(self, x, adj_mat, edge_index, edge_labels, val_mask, criterion):\n",
    "        \"\"\"Validate with mini-batch processing\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        val_indices = torch.where(val_mask)[0]\n",
    "        n_val = len(val_indices)\n",
    "        \n",
    "        batch_size = 7000\n",
    "        n_batches = (n_val + batch_size - 1) // batch_size\n",
    "        \n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(n_batches):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min((i + 1) * batch_size, n_val)\n",
    "                batch_indices = val_indices[start_idx:end_idx]\n",
    "                \n",
    "                batch_edge_index = edge_index[:, batch_indices]\n",
    "                batch_labels = edge_labels[batch_indices]\n",
    "                \n",
    "                out = self.model(x, adj_mat, batch_edge_index)\n",
    "                loss = criterion(out, batch_labels)\n",
    "                \n",
    "                total_loss += loss.item() * len(batch_indices)\n",
    "                pred = out.max(1)[1]\n",
    "                all_preds.extend(pred.cpu().numpy())\n",
    "                all_labels.extend(batch_labels.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / n_val\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        return avg_loss, acc, f1\n",
    "    \n",
    "    def train(self, x, adj_mat, edge_index, edge_labels, train_mask, val_mask,\n",
    "              epochs=200, lr=0.005, weight_decay=5e-4, patience=30):\n",
    "        \"\"\"\n",
    "        Complete training loop\n",
    "        \"\"\"\n",
    "        print(f\"\\nâ™»Training GAT-DDI Model\")\n",
    "        print(f\"â€¢ Device: {self.device}\")\n",
    "        print(f\"â€¢ Parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
    "        print(f\"â€¢ Training edges: {train_mask.sum()}\")\n",
    "        print(f\"â€¢ Validation edges: {val_mask.sum()}\")\n",
    "        \n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        best_val_acc = 0\n",
    "        best_val_f1 = 0\n",
    "        patience_counter = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        print(\"\\n\" + \"_\"*80)\n",
    "        print(\"EPOCH | TRAIN LOSS | TRAIN ACC | TRAIN F1 | VAL LOSS | VAL ACC | VAL F1\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            train_loss, train_acc, train_f1 = self.train_epoch(\n",
    "                x, adj_mat, edge_index, edge_labels, train_mask, optimizer, criterion\n",
    "            )\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, val_acc, val_f1 = self.validate(\n",
    "                x, adj_mat, edge_index, edge_labels, val_mask, criterion\n",
    "            )\n",
    "            \n",
    "            # Store metrics\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['val_acc'].append(val_acc)\n",
    "            self.history['train_f1'].append(train_f1)\n",
    "            self.history['val_f1'].append(val_f1)\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_acc = val_acc\n",
    "                best_val_f1 = val_f1\n",
    "                patience_counter = 0\n",
    "                torch.save(self.model.state_dict(), f'models/best_GAT_DDI_{timestamp}.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Print progress\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"{epoch+1:5d} | {train_loss:10.4f} | {train_acc:9.4f} | \"\n",
    "                      f\"{train_f1:8.4f} | {val_loss:8.4f} | {val_acc:7.4f} | {val_f1:6.4f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nâŠ˜ Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        print(\"-\"*75)\n",
    "        print(f\"(âœ“) Training completed in {training_time:.2f} seconds\")\n",
    "        print(f\"â˜… Best validation accuracy: {best_val_acc:.4f}\")\n",
    "        print(f\"â˜… Best validation F1-score: {best_val_f1:.4f}\")\n",
    "        print(\"_\"*75)\n",
    "        \n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data, device):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data['node_features'], data['adj_mat'], data['edge_index'])\n",
    "        pred = out[data['test_mask']].max(1)[1]\n",
    "        y_true = data['edge_labels'][data['test_mask']].cpu().numpy()\n",
    "        y_pred = pred.cpu().numpy()\n",
    "        \n",
    "        # Metrics\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ Test Accuracy: {acc:.4f}\")\n",
    "        print(f\"ðŸŽ¯ Test F1-Score: {f1:.4f}\")\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Classification Report:\")\n",
    "        print(classification_report(y_true, y_pred, \n",
    "                                   target_names=[f'Type {i}' for i in range(data['n_classes'])],\n",
    "                                   zero_division=0))\n",
    "        \n",
    "        return {'accuracy': acc, 'f1_score': f1, 'predictions': y_pred, 'true_labels': y_true}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plotting and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, save_path=f'images/DDI_training_history_{timestamp}.png'):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(history['train_loss'], label='Train', linewidth=2)\n",
    "    axes[0].plot(history['val_loss'], label='Validation', linewidth=2)\n",
    "    axes[0].set_title('Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].plot(history['train_acc'], label='Train', linewidth=2)\n",
    "    axes[1].plot(history['val_acc'], label='Validation', linewidth=2)\n",
    "    axes[1].set_title('Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # F1-Score\n",
    "    axes[2].plot(history['train_f1'], label='Train', linewidth=2)\n",
    "    axes[2].plot(history['val_f1'], label='Validation', linewidth=2)\n",
    "    axes[2].set_title('F1-Score', fontsize=14, fontweight='bold')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('F1-Score')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"âœ“ Saved training history to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "data = prepare_ddi_data()\n",
    "\n",
    "# Move to device\n",
    "for key in ['node_features', 'edge_index', 'edge_labels', 'adj_mat', \n",
    "            'train_mask', 'val_mask', 'test_mask']:\n",
    "    data[key] = data[key].to(device)\n",
    "\n",
    "# Model configuration\n",
    "config = {\n",
    "    'n_features': data['node_features'].shape[1],\n",
    "    'n_hidden': 256,\n",
    "    'n_classes': data['n_classes'],\n",
    "    'n_heads': 8,\n",
    "    'dropout': 0.3\n",
    "}\n",
    "print('_'*50)\n",
    "print(f\"\\nâ›¯ Model Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"â€¢ {key}: {value}\")\n",
    "print('_'*50)\n",
    "# Initialize model\n",
    "model = GAT_DDI(**config)\n",
    "trainer = DDITrainer(model, device)\n",
    "\n",
    "# Train\n",
    "history = trainer.train(\n",
    "    data['node_features'], data['adj_mat'], data['edge_index'],\n",
    "    data['edge_labels'], data['train_mask'], data['val_mask'],\n",
    "    epochs=50, lr=0.001, weight_decay=5e-4, patience=30\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned embeddings\n",
    "print(\"Generating embedding visualization...\")\n",
    "evaluator.visualize_embeddings(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate\n",
    "model.load_state_dict(torch.load(f'models/best_GAT_DDI_{timestamp}.pth'))\n",
    "results = evaluate_model(model, data, device)\n",
    "\n",
    "# Save results\n",
    "torch.save({\n",
    "    'config': config,\n",
    "    'history': history,\n",
    "    'results': results,\n",
    "    'timestamp': timestamp\n",
    "}, f'results/GAT_DDI_results_{timestamp}.pth')\n",
    "\n",
    "# save_file(model.state_dict(), f'models/best_GAT_DDI_{timestamp}.safetensors')\n",
    "\n",
    "print(f\"\\nâœ… All results saved!\")\n",
    "print(f\"   â€¢ Model: models/best_GAT_DDI_{timestamp}.pth\")\n",
    "print(f\"   â€¢ Results: results/GAT_DDI_results_{timestamp}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Saving Logs and Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare the new log entry\n",
    "current_log = {\n",
    "    \"date\": timestamp,\n",
    "    \"graph_details\": {\n",
    "        \"Number of edges\": len(data['edge_index'][0]), # Assuming edge_index format\n",
    "        \"Number of interaction types\": data['n_classes'],\n",
    "        \"Feature dimension\": data['node_features'].shape[1]\n",
    "    },\n",
    "    \"config\": config,\n",
    "    \"runtime_info\": {\n",
    "        \"device\": str(device),\n",
    "        \"parameters\": sum(p.numel() for p in model.parameters()),\n",
    "        \"training_edges\": int(data['train_mask'].sum()),\n",
    "        \"validation_edges\": int(data['val_mask'].sum())\n",
    "    },\n",
    "    \"metrics\": {\n",
    "        \"train_loss\": history['train_loss'][-1],\n",
    "        \"train_acc\": history['train_acc'][-1],\n",
    "        \"train_f1\": history.get('train_f1', [-1])[-1], # .get() prevents errors if missing\n",
    "        \"val_loss\": history['val_loss'][-1],\n",
    "        \"val_acc\": history['val_acc'][-1],\n",
    "        \"val_f1\": history.get('val_f1', [-1])[-1]\n",
    "    }\n",
    "}\n",
    "\n",
    "# 2. File handling: Load existing data or create a new list\n",
    "log_file = 'model_evals_log.json'\n",
    "\n",
    "if os.path.exists(log_file):\n",
    "    with open(log_file, 'r') as f:\n",
    "        try:\n",
    "            logs_list = json.load(f)\n",
    "            if not isinstance(logs_list, list):\n",
    "                logs_list = []\n",
    "        except json.JSONDecodeError:\n",
    "            logs_list = []\n",
    "else:\n",
    "    logs_list = []\n",
    "\n",
    "# 3. Insert new log at the TOP (index 0)\n",
    "logs_list.insert(0, new_log)\n",
    "\n",
    "# 4. Save back to file\n",
    "with open(log_file, 'w') as f:\n",
    "    json.dump(logs_list, f, indent=4)\n",
    "\n",
    "print(f\"ðŸš€ Log successfully prepended to {log_file}!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "# 1. Initialize the model architecture with the same config\n",
    "# (Ensure GAT_DDI and config are defined in your current session)\n",
    "test_model = GAT_DDI(**config)\n",
    "\n",
    "# 2. Load weights using safetensors\n",
    "weights_path = f'models/best_GAT_DDI_{timestamp}.safetensors'\n",
    "state_dict = load_file(weights_path)\n",
    "test_model.load_state_dict(state_dict)\n",
    "\n",
    "# 3. Prepare for inference\n",
    "test_model.to(device)\n",
    "test_model.eval()  # CRITICAL: Sets layers like Dropout to evaluation mode\n",
    "\n",
    "# 4. Test with input (Forward Pass)\n",
    "with torch.no_grad():  # Disables gradient calculation for faster inference\n",
    "    # Using your existing data object as a test input\n",
    "    test_output = test_model(\n",
    "        data['node_features'], \n",
    "        data['adj_mat'], \n",
    "        data['edge_index']\n",
    "    )\n",
    "\n",
    "# 5. Quick Verification\n",
    "print(f\"âœ… Model loaded from: {weights_path}\")\n",
    "print(f\"ðŸ“¡ Output Shape: {test_output.shape}\")\n",
    "print(f\"ðŸŽ¯ Sample Prediction: {torch.softmax(test_output[0], dim=0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_matrix, class_names, save_path=f'images/GAT_confusion_matrix_{timestamp}.png'):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix with better styling\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Predicted Class', fontsize=12)\n",
    "    plt.ylabel('True Class', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrix\n",
    "class_names = [f'Class {i}' for i in range(dataset.num_classes)]\n",
    "plot_confusion_matrix(test_results['confusion_matrix'], class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation class for GAT models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: GAT, device: torch.device):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "    \n",
    "    def test(self, data):\n",
    "        \"\"\"\n",
    "        Comprehensive testing with multiple metrics\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = self.model(data.x, data.adj_mat)\n",
    "            pred = out[data.test_mask].max(1)[1]\n",
    "            y_true = data.y[data.test_mask].cpu().numpy()\n",
    "            y_pred = pred.cpu().numpy()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            test_acc = accuracy_score(y_true, y_pred)\n",
    "            class_report = classification_report(y_true, y_pred, output_dict=True)\n",
    "            conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "            \n",
    "            return {\n",
    "                'accuracy': test_acc,\n",
    "                'classification_report': class_report,\n",
    "                'confusion_matrix': conf_matrix,\n",
    "                'predictions': y_pred,\n",
    "                'true_labels': y_true\n",
    "            }\n",
    "    \n",
    "    def visualize_embeddings(self, data, save_path=f'images/GAT_visualize_embeddings_{timestamp}.png'):\n",
    "        \"\"\"\n",
    "        Visualize node embeddings using t-SNE\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Get embeddings from first layer\n",
    "            x = self.model.dropout_layer(data.x)\n",
    "            embeddings = self.model.attention1(x, data.adj_mat)\n",
    "            embeddings = embeddings.cpu().numpy()\n",
    "            \n",
    "            # Apply t-SNE\n",
    "            print(\"ðŸ”„ Computing t-SNE embeddings...\")\n",
    "            tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "            embeddings_2d = tsne.fit_transform(embeddings)\n",
    "            \n",
    "            # Plot\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "                                c=data.y.cpu().numpy(), cmap='tab10', alpha=0.7, s=50)\n",
    "            plt.colorbar(scatter, label='Node Class')\n",
    "            plt.title('GAT Node Embeddings Visualization (t-SNE)', fontsize=16, fontweight='bold')\n",
    "            plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "            plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "\n",
    "# Load best model and evaluate\n",
    "model.load_state_dict(torch.load(f'best_GAT_model_{timestamp}.pth'))\n",
    "\n",
    "# weights = load_file(f'models/best_GAT_DDI_{timestamp}.safetensors')\n",
    "# model.load_state_dict(weights)\n",
    "\n",
    "evaluator = ModelEvaluator(model, device)\n",
    "\n",
    "test_results = evaluator.test(data)\n",
    "\n",
    "print(f\"ðŸŽ¯ Test Result Accuracy: {test_results['accuracy']:.4f}\")\n",
    "print(\"\\nðŸ“Š Classification Report:\")\n",
    "print(classification_report(test_results['true_labels'], test_results['predictions']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model performance\n",
    "def analyze_results(history, test_results):\n",
    "    \"\"\"\n",
    "    Provide comprehensive analysis of model performance\n",
    "    \"\"\"\n",
    "    print(\"ðŸ“ˆ MODEL PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Training metrics\n",
    "    print(f\"\\nTraining Metrics:\")\n",
    "    print(f\"â€¢ Final Training Accuracy: {history['train_accuracies'][-1]:.4f}\")\n",
    "    print(f\"â€¢ Final Validation Accuracy: {history['val_accuracies'][-1]:.4f}\")\n",
    "    print(f\"â€¢ Best Validation Accuracy: {history['best_val_acc']:.4f}\")\n",
    "    print(f\"â€¢ Training Time: {history['training_time']:.2f} seconds\")\n",
    "    print(f\"â€¢ Total Epochs: {len(history['train_losses'])}\")\n",
    "    \n",
    "    # Test metrics\n",
    "    print(f\"Test Metrics:\")\n",
    "    print(f\"â€¢ Test Accuracy: {test_results['accuracy']:.4f}\")\n",
    "    \n",
    "    # Per-class performance\n",
    "    print(f\"\\nðŸ“Š Per-Class Performance:\")\n",
    "    for i, (precision, recall, f1) in enumerate(zip(\n",
    "        [test_results['classification_report'][str(i)]['precision'] for i in range(dataset.num_classes)],\n",
    "        [test_results['classification_report'][str(i)]['recall'] for i in range(dataset.num_classes)],\n",
    "        [test_results['classification_report'][str(i)]['f1-score'] for i in range(dataset.num_classes)]\n",
    "    )):\n",
    "        print(f\"   â€¢ Class {i}: Precision={precision:.3f}, Recall={recall:.3f}, F1={f1:.3f}\")\n",
    "    \n",
    "    # Model complexity\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\nðŸ”§ Model Complexity:\")\n",
    "    print(f\"   â€¢ Total Parameters: {total_params:,}\")\n",
    "    print(f\"   â€¢ Trainable Parameters: {trainable_params:,}\")\n",
    "    print(f\"   â€¢ Model Size: ~{total_params * 4 / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Run analysis\n",
    "analyze_results(history, test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive results\n",
    "results_summary = {\n",
    "    'model_config': config,\n",
    "    'training_history': history,\n",
    "    'test_results': {\n",
    "        'accuracy': test_results['accuracy'],\n",
    "        'classification_report': test_results['classification_report']\n",
    "    },\n",
    "    'dataset_info': {\n",
    "        'name': 'Cora',\n",
    "        'num_nodes': data.x.size(0),\n",
    "        'num_edges': data.edge_index.size(1),\n",
    "        'num_features': data.x.size(1),\n",
    "        'num_classes': dataset.num_classes\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(results_summary, 'models/gat_complete_results.pth')\n",
    "print(\"\\n Results saved to 'gat_complete_results.pth'ðŸ—¹\")\n",
    "print(\"Best model saved to 'best_gat_model.pth' ðŸ—¹\")\n",
    "\n",
    "print(\"_\"*50)\n",
    "print(f\"ðŸ† Final Test Accuracy: {test_results['accuracy']:.4f}\")\n",
    "print(f\"â±ï¸ Total Training Time: {history['training_time']:.2f} seconds\")\n",
    "print(f\"ðŸŽ¯ Best Validation Accuracy: {history['best_val_acc']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
