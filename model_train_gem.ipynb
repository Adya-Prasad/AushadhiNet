{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f768b07",
   "metadata": {},
   "source": [
    "GraphRX\" methodology: Molecular Encoder + GNN Topology Learning\n",
    "| Feature | Specification |\n",
    "| :--- | :--- |\n",
    "| **Problem Type** | Link Prediction |\n",
    "| **Classes** | 2 (Binary: safe/interact) |\n",
    "| **Loss Function** | `BCEWithLogitsLoss` |\n",
    "| **Architecture** | PyTorch Geometric `GATv2Conv` |\n",
    "| **Training Edges** | 97,028 positive edges + negatives |\n",
    "| **Complexity** | Lower |\n",
    "| **Training Time** | ~15 min for 150 epochs |\n",
    "\n",
    "```bash\n",
    "# PyTorch Geometric - LINK PREDICTION\n",
    "class ModelArchitecture:\n",
    "    - Uses GATv2Conv (optimized C++ backend)\n",
    "    - 3 layers: 128 ‚Üí 64 ‚Üí 64\n",
    "    - Dot product decoder: z[src] ¬∑ z[dst] ‚Üí probability\n",
    "    - Binary output: safe (0) or interaction (1)\n",
    "```\n",
    "\n",
    "**Why it's fast:**\n",
    "\n",
    "-  Optimized library - GATv2Conv is highly optimized\n",
    "- Binary classification - only 2 classes (much simpler)\n",
    "-  Smaller model - 128‚Üí64‚Üí64 vs 1032‚Üí256‚Üí256‚Üí128\n",
    "-  Dot product decoder - simple operation\n",
    "-  Negative sampling - doesn't process all edges\n",
    "\n",
    "**Strengths:**\n",
    "\n",
    "-  Fast training\n",
    "-  Simple deployment\n",
    "-  Good for screening (safe vs unsafe)\n",
    "-  Industry-standard approach\n",
    "\n",
    "### For EACH epoch:\n",
    "1. Single forward pass through entire graph (GATv2Conv optimized)\n",
    "2. Sample negative edges (fast operation)\n",
    "3. Compute dot products for pos/neg edges\n",
    "4. Binary loss (BCEWithLogitsLoss)\n",
    "5. Single backward pass\n",
    "\n",
    "# Per epoch: 1 pass √ó optimized ops = FAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8b93a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv, BatchNorm\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, rdFingerprintGenerator\n",
    "import torch.nn.functional as F\n",
    "from safetensors.torch import save_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04237c9c",
   "metadata": {},
   "source": [
    "### 2. System Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54e06d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "GPU: NVIDIA GeForce GTX 1650\n",
      "GPU Memory: 4.00 GB\n",
      "‚úì Memory optimization enabled\n",
      "‚úì Directory `images/` exists \n",
      "‚úì Directory `models/` exists \n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"DEVICE: {DEVICE}\")\n",
    "# Memory optimization settings for GPU, Aggressive memory management\n",
    "if DEVICE.type == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(\"‚úì Memory optimization enabled\")\n",
    "\n",
    "# specific style date time saving\n",
    "timestamp = datetime.now().strftime(\"%d_%b_%H-%M\")\n",
    "\n",
    "# validating required directories:\n",
    "required_directories = ['images', 'models']\n",
    "for folder in required_directories:\n",
    "    if not os.path.exists(folder):\n",
    "        print(f\"‚úò Directory `{folder}/` not found  Creating...\")\n",
    "        os.makedirs(folder)\n",
    "    else:\n",
    "        print(f\"‚úì Directory `{folder}/` exists \")\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab83d6f2",
   "metadata": {},
   "source": [
    "### 2.2 Custom Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "322064ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Customized Configurations Initialized on cuda\n"
     ]
    }
   ],
   "source": [
    "# Config matches the paper's \"Safe Pairing\" focus\n",
    "CONFIG = {\n",
    "    'MODEL_PATH': 'models/Gemini_AushadiNet_GATv2_128',\n",
    "    'DATA_PATH': 'dataset/drugdata', \n",
    "    'INTERACTION_FILE': 'ddis.csv',\n",
    "    'SMILES_FILE': 'drug_smiles.csv',\n",
    "    'NODE_DIM': 128,      # Paper uses 128-bit embeddings\n",
    "    'HIDDEN_DIM': 128,     # Hidden layers\n",
    "    'OUTPUT_DIM': 64,     # Latent space\n",
    "    'HEADS': 4,           # Multi-head attention (Critical for accuracy)\n",
    "    'DROPOUT': 0.2,       # Regularization\n",
    "    'LR': 0.005,\n",
    "    'EPOCHS': 150,        # Extended training for convergence\n",
    "    'DEVICE': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "}\n",
    "\n",
    "print(f\"‚úì Customized Configurations Initialized on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5c0fd4",
   "metadata": {},
   "source": [
    "### 3. Data Loading Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f0fd403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPLORING DRUG-DRUG INTERACTION DATASET:\n",
      "\n",
      "DDI Dataset Shape: (191808, 4)\n",
      "Columns: ['d1', 'd2', 'type', 'Neg samples']\n",
      "\n",
      "üî¨ Interaction Types Distribution:\n",
      "type\n",
      "48    60751\n",
      "46    34360\n",
      "72    23779\n",
      "74     9470\n",
      "59     8397\n",
      "      ...  \n",
      "42       11\n",
      "61       11\n",
      "51       10\n",
      "25        7\n",
      "41        6\n",
      "Name: count, Length: 86, dtype: int64\n",
      "\n",
      "üìã Sample DDI Data:\n",
      "        d1       d2  type Neg samples\n",
      "0  DB04571  DB00460     0   DB01579$t\n",
      "1  DB00855  DB00460     0   DB01178$t\n",
      "2  DB09536  DB00460     0   DB06626$t\n",
      "3  DB01600  DB00460     0   DB01588$t\n",
      "4  DB09000  DB00460     0   DB06196$t\n",
      "5  DB11630  DB00460     0   DB00744$t\n",
      "6  DB00553  DB00460     0   DB06413$t\n",
      "7  DB06261  DB00460     0   DB00876$t\n",
      "8  DB01878  DB00460     0   DB09267$t\n",
      "9  DB00140  DB00460     0   DB01204$t\n",
      "\n",
      "üíä Drug SMILES Dataset Shape: (1706, 2)\n",
      "Columns: ['drug_id', 'smiles']\n",
      "\n",
      "üìã Sample SMILES Data:\n",
      "   drug_id                                             smiles\n",
      "0  DB04571                CC1=CC2=CC3=C(OC(=O)C=C3C)C(C)=C2O1\n",
      "1  DB00855                                    NCC(=O)CCC(O)=O\n",
      "2  DB09536                                           O=[Ti]=O\n",
      "3  DB01600              CC(C(O)=O)C1=CC=C(S1)C(=O)C1=CC=CC=C1\n",
      "4  DB09000         CC(CN(C)C)CN1C2=CC=CC=C2SC2=C1C=C(C=C2)C#N\n",
      "5  DB11630  Oc1cccc(-c2c3nc(c(-c4cccc(O)c4)c4ccc([nH]4)c(-...\n",
      "6  DB00553                     COC1=C2OC(=O)C=CC2=CC2=C1OC=C2\n",
      "7  DB06261                      [H]N([H])CC(=O)CCC(=O)OCCCCCC\n",
      "8  DB01878                        O=C(C1=CC=CC=C1)C1=CC=CC=C1\n",
      "9  DB00140  CC1=C(C)C=C2N(C[C@H](O)[C@H](O)[C@H](O)CO)C3=N...\n",
      "\n",
      "üìà Statistics:\n",
      "‚Ä¢ Total DDI pairs: 191808\n",
      "‚Ä¢ Unique drugs in DDI: 1706\n",
      "‚Ä¢ Drugs with SMILES: 1706\n",
      "‚Ä¢ Interaction type 0: 11\n",
      "‚Ä¢ Interaction type 1: 323\n",
      "‚Ä¢ Drugs with both DDI and SMILES: 1706\n",
      "‚Ä¢ Coverage: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Load DDI data\n",
    "print(\"EXPLORING DRUG-DRUG INTERACTION DATASET:\\n\")\n",
    "\n",
    "# Load DDI interactions\n",
    "ddi_df = pd.read_csv('dataset/drugdata/ddis.csv')\n",
    "print(f\"DDI Dataset Shape: {ddi_df.shape}\")\n",
    "print(f\"Columns: {ddi_df.columns.tolist()}\")\n",
    "print(f\"\\nüî¨ Interaction Types Distribution:\")\n",
    "print(ddi_df['type'].value_counts())\n",
    "print(f\"\\nüìã Sample DDI Data:\")\n",
    "print(ddi_df.head(10))\n",
    "\n",
    "# Load drug SMILES\n",
    "smiles_df = pd.read_csv('dataset/drugdata/drug_smiles.csv')\n",
    "print(f\"\\nüíä Drug SMILES Dataset Shape: {smiles_df.shape}\")\n",
    "print(f\"Columns: {smiles_df.columns.tolist()}\")\n",
    "print(f\"\\nüìã Sample SMILES Data:\")\n",
    "print(smiles_df.head(10))\n",
    "\n",
    "# Get unique drugs\n",
    "unique_drugs_ddi = set(ddi_df['d1'].unique()) | set(ddi_df['d2'].unique())\n",
    "print(f\"\\nüìà Statistics:\")\n",
    "print(f\"‚Ä¢ Total DDI pairs: {len(ddi_df)}\")\n",
    "print(f\"‚Ä¢ Unique drugs in DDI: {len(unique_drugs_ddi)}\")\n",
    "print(f\"‚Ä¢ Drugs with SMILES: {len(smiles_df)}\")\n",
    "print(f\"‚Ä¢ Interaction type 0: {(ddi_df['type'] == 0).sum()}\")\n",
    "print(f\"‚Ä¢ Interaction type 1: {(ddi_df['type'] == 1).sum()}\")\n",
    "\n",
    "# Check overlap\n",
    "drugs_with_smiles = set(smiles_df['drug_id'].unique())\n",
    "overlap = unique_drugs_ddi & drugs_with_smiles\n",
    "print(f\"‚Ä¢ Drugs with both DDI and SMILES: {len(overlap)}\")\n",
    "print(f\"‚Ä¢ Coverage: {len(overlap)/len(unique_drugs_ddi)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "551e4584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading datasets...\n",
      "‚öóÔ∏è Generating features for 1706 drugs...\n",
      "üè∑Ô∏è Encoding Interaction Types...\n",
      "‚úÖ Data Ready: 191808 interactions, 86 unique interaction types.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. ROBUST DATA LOADER ---\n",
    "class MulticlassDataLoader:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.drug_map = {} \n",
    "        self.label_encoder = LabelEncoder()\n",
    "        # Initialize Fingerprint Generator once\n",
    "        self.fp_gen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=config['NODE_DIM'])\n",
    "\n",
    "    def get_molecular_features(self, smiles):\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None: \n",
    "                return np.zeros((self.config['NODE_DIM'],), dtype=np.float32)\n",
    "            return self.fp_gen.GetFingerprintAsNumPy(mol).astype(np.float32)\n",
    "        except:\n",
    "            return np.zeros((self.config['NODE_DIM'],), dtype=np.float32)\n",
    "\n",
    "    def load_data(self):\n",
    "        print(\"üì• Loading datasets...\")\n",
    "        \n",
    "        ddi_path = os.path.join(self.config['DATA_PATH'], self.config['INTERACTION_FILE'])\n",
    "        smiles_path = os.path.join(self.config['DATA_PATH'], self.config['SMILES_FILE'])\n",
    "\n",
    "        # 1. Load Interaction File (Robust Check)\n",
    "        if not os.path.exists(ddi_path):\n",
    "            raise FileNotFoundError(f\"‚ùå File not found: {ddi_path}. Please ensure 'ddis.csv' is in the dataset folder.\")\n",
    "            \n",
    "        # We assume ddis.csv is comma-separated based on previous context\n",
    "        ddi_df = pd.read_csv(ddi_path)\n",
    "        \n",
    "        # Validation: Check if 'type' column exists for Multiclass\n",
    "        if 'type' not in ddi_df.columns:\n",
    "            # Fallback for TSV/No-Header files (Not recommended for Multiclass, but prevents crash)\n",
    "            print(\"‚ö†Ô∏è Warning: 'type' column missing! Attempting to read as headerless TSV...\")\n",
    "            ddi_df = pd.read_csv(ddi_path, sep='\\t', names=['d1', 'd2'])\n",
    "            if 'type' not in ddi_df.columns:\n",
    "                raise ValueError(\"‚ùå CRITICAL ERROR: The dataset provided does not have a 'type' column. \"\n",
    "                                 \"You cannot perform Multiclass Classification without interaction types. \"\n",
    "                                 \"Please use 'ddis.csv' instead of the TSV file.\")\n",
    "\n",
    "        smiles_df = pd.read_csv(smiles_path)\n",
    "        \n",
    "        # 2. Map Drug IDs -> Integer Indices\n",
    "        all_drugs = set(ddi_df['d1']).union(set(ddi_df['d2'])).union(set(smiles_df['drug_id']))\n",
    "        self.drug_map = {d: i for i, d in enumerate(all_drugs)}\n",
    "        num_nodes = len(all_drugs)\n",
    "        \n",
    "        # 3. Create Node Features (X)\n",
    "        print(f\"‚öóÔ∏è Generating features for {num_nodes} drugs...\")\n",
    "        x = np.zeros((num_nodes, self.config['NODE_DIM']), dtype=np.float32)\n",
    "        smiles_dict = dict(zip(smiles_df.drug_id, smiles_df.smiles))\n",
    "        \n",
    "        for drug_id, idx in self.drug_map.items():\n",
    "            if drug_id in smiles_dict:\n",
    "                x[idx] = self.get_molecular_features(smiles_dict[drug_id])\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "\n",
    "        # 4. Process Edges & Labels\n",
    "        print(\"üè∑Ô∏è Encoding Interaction Types...\")\n",
    "        \n",
    "        # Filter valid drugs\n",
    "        valid_mask = ddi_df['d1'].isin(self.drug_map) & ddi_df['d2'].isin(self.drug_map)\n",
    "        clean_df = ddi_df[valid_mask].copy()\n",
    "\n",
    "        # Encode the 'type' column (e.g., 48 -> 0, 72 -> 1)\n",
    "        clean_df['encoded_type'] = self.label_encoder.fit_transform(clean_df['type'])\n",
    "        num_classes = len(self.label_encoder.classes_)\n",
    "        \n",
    "        # Build Edge Index\n",
    "        src = [self.drug_map[d] for d in clean_df['d1']]\n",
    "        dst = [self.drug_map[d] for d in clean_df['d2']]\n",
    "        edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "        \n",
    "        # Build Labels\n",
    "        edge_attr = torch.tensor(clean_df['encoded_type'].values, dtype=torch.long)\n",
    "\n",
    "        print(f\"‚úÖ Data Ready: {len(clean_df)} interactions, {num_classes} unique interaction types.\")\n",
    "        \n",
    "        data = Data(x=x, edge_index=edge_index, y=edge_attr)\n",
    "        return data, self.drug_map, num_classes, self.label_encoder\n",
    "\n",
    "# Run Loader\n",
    "loader = MulticlassDataLoader(CONFIG)\n",
    "data, drug_map, num_classes, label_encoder = loader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c7186",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73022658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Architecture Upgraded: MLP Decoder for Multiclass Prediction\n",
      "AushadhiNetMulticlass(\n",
      "  (conv1): GATv2Conv(128, 128, heads=4)\n",
      "  (bn1): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): GATv2Conv(512, 128, heads=4)\n",
      "  (bn2): BatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): GATv2Conv(512, 64, heads=1)\n",
      "  (skip): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=64, out_features=86, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1357: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:35.)\n",
      "  return t.to(\n"
     ]
    }
   ],
   "source": [
    "class AushadhiNetMulticlass(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, num_classes, heads=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- ENCODER (The Graph Brain) ---\n",
    "        # Learns \"Who is connected to whom\" and \"What are they made of\"\n",
    "        self.conv1 = GATv2Conv(in_dim, hidden_dim, heads=heads, dropout=dropout, concat=True)\n",
    "        self.bn1 = BatchNorm(hidden_dim * heads)\n",
    "        \n",
    "        self.conv2 = GATv2Conv(hidden_dim * heads, hidden_dim, heads=heads, dropout=dropout, concat=True)\n",
    "        self.bn2 = BatchNorm(hidden_dim * heads)\n",
    "        \n",
    "        self.conv3 = GATv2Conv(hidden_dim * heads, out_dim, heads=1, dropout=dropout, concat=False)\n",
    "        self.skip = torch.nn.Linear(in_dim, out_dim)\n",
    "\n",
    "        # --- DECODER (The Classifier Head) ---\n",
    "        # Instead of dot product, we Concatenate embeddings -> MLP -> Softmax\n",
    "        # Input dim is out_dim * 2 because we concat Drug A and Drug B\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(out_dim * 2, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim // 2, num_classes) # Outputs logits for 86 classes\n",
    "        )\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        # 1. Graph Processing\n",
    "        identity = self.skip(x)\n",
    "        x = F.elu(self.bn1(self.conv1(x, edge_index)))\n",
    "        x = F.elu(self.bn2(self.conv2(x, edge_index)))\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x + identity\n",
    "\n",
    "    def decode(self, z, edge_index):\n",
    "        # 2. Extract Embeddings for Source and Dest nodes\n",
    "        src_emb = z[edge_index[0]]\n",
    "        dst_emb = z[edge_index[1]]\n",
    "        \n",
    "        # 3. Concatenate Features (Drug A || Drug B)\n",
    "        edge_feat = torch.cat([src_emb, dst_emb], dim=1)\n",
    "        \n",
    "        # 4. Classify Interaction Type\n",
    "        return self.classifier(edge_feat)\n",
    "\n",
    "model = AushadhiNetMulticlass(\n",
    "    in_dim=CONFIG['NODE_DIM'],\n",
    "    hidden_dim=CONFIG['HIDDEN_DIM'],\n",
    "    out_dim=CONFIG['OUTPUT_DIM'],\n",
    "    num_classes=num_classes,\n",
    "    heads=CONFIG['HEADS']\n",
    ").to(CONFIG['DEVICE'])\n",
    "\n",
    "print(\"üß† Architecture Upgraded: MLP Decoder for Multiclass Prediction\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b36362e",
   "metadata": {},
   "source": [
    "### Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26b48956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Optimizer & Loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['LR'], weight_decay=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss() # Multiclass Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af861609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Multiclass Training Protocol...\n",
      "Epoch 010 | Loss: 2.6559 | Val Accuracy: 34.14%\n",
      "Epoch 020 | Loss: 2.5450 | Val Accuracy: 31.85%\n",
      "Epoch 030 | Loss: 2.3811 | Val Accuracy: 37.46%\n",
      "Epoch 040 | Loss: 2.2295 | Val Accuracy: 40.05%\n",
      "Epoch 050 | Loss: 1.9998 | Val Accuracy: 44.51%\n",
      "Epoch 060 | Loss: 1.8692 | Val Accuracy: 46.99%\n",
      "Epoch 070 | Loss: 1.7110 | Val Accuracy: 50.65%\n",
      "Epoch 080 | Loss: 1.5667 | Val Accuracy: 56.51%\n",
      "Epoch 090 | Loss: 1.4931 | Val Accuracy: 56.92%\n",
      "Epoch 100 | Loss: 1.4527 | Val Accuracy: 57.82%\n",
      "Epoch 110 | Loss: 1.2802 | Val Accuracy: 63.66%\n",
      "Epoch 120 | Loss: 1.1785 | Val Accuracy: 65.66%\n",
      "Epoch 130 | Loss: 1.1167 | Val Accuracy: 67.15%\n",
      "Epoch 140 | Loss: 1.0545 | Val Accuracy: 69.01%\n",
      "Epoch 150 | Loss: 0.9840 | Val Accuracy: 71.45%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split Data (Train on known interactions to learn types)\n",
    "# We split the *edges* themselves\n",
    "num_edges = data.edge_index.size(1)\n",
    "perm = torch.randperm(num_edges)\n",
    "\n",
    "train_size = int(0.8 * num_edges)\n",
    "val_size = int(0.1 * num_edges)\n",
    "\n",
    "train_idx = perm[:train_size]\n",
    "val_idx = perm[train_size:train_size + val_size]\n",
    "test_idx = perm[train_size + val_size:]\n",
    "\n",
    "print(\"üöÄ Starting Multiclass Training Protocol...\")\n",
    "history = {'loss': [], 'acc': []}\n",
    "\n",
    "for epoch in range(1, CONFIG['EPOCHS'] + 1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 1. Get Node Embeddings (using whole graph structure)\n",
    "    z = model.encode(data.x.to(CONFIG['DEVICE']), data.edge_index.to(CONFIG['DEVICE']))\n",
    "    \n",
    "    # 2. Predict Classes for Training Edges\n",
    "    # We only train on the edges in the training set\n",
    "    train_edges = data.edge_index[:, train_idx].to(CONFIG['DEVICE'])\n",
    "    train_labels = data.y[train_idx].to(CONFIG['DEVICE'])\n",
    "    \n",
    "    out = model.decode(z, train_edges)\n",
    "    \n",
    "    # 3. Compute Loss\n",
    "    loss = criterion(out, train_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 4. Validation\n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_edges = data.edge_index[:, val_idx].to(CONFIG['DEVICE'])\n",
    "            val_labels = data.y[val_idx].to(CONFIG['DEVICE'])\n",
    "            \n",
    "            val_out = model.decode(z, val_edges)\n",
    "            # Get predicted class (argmax)\n",
    "            preds = val_out.argmax(dim=1)\n",
    "            \n",
    "            acc = accuracy_score(val_labels.cpu(), preds.cpu())\n",
    "            history['loss'].append(loss.item())\n",
    "            history['acc'].append(acc)\n",
    "            \n",
    "            print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Val Accuracy: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5cbe41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ Final Test Performance:\n",
      "Accuracy: 0.7117\n",
      "Macro F1 Score: 0.2824\n",
      "üîí Weights saved: models/Gemini_AushadiNet_GATv2_128.safetensors\n",
      "üìú Metadata & Class Mappings saved: models/Gemini_AushadiNet_GATv2_128_config.json\n",
      "‚úÖ AushadhiNet Multiclass Upgrade Complete.\n"
     ]
    }
   ],
   "source": [
    "# 1. Final Evaluation\n",
    "model.eval()\n",
    "z = model.encode(data.x.to(CONFIG['DEVICE']), data.edge_index.to(CONFIG['DEVICE']))\n",
    "test_edges = data.edge_index[:, test_idx].to(CONFIG['DEVICE'])\n",
    "test_labels = data.y[test_idx].to(CONFIG['DEVICE'])\n",
    "\n",
    "logits = model.decode(z, test_edges)\n",
    "preds = logits.argmax(dim=1)\n",
    "\n",
    "print(\"\\nüèÜ Final Test Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(test_labels.cpu(), preds.cpu()):.4f}\")\n",
    "# Macro F1 is better for imbalanced datasets (like medical data)\n",
    "print(f\"Macro F1 Score: {f1_score(test_labels.cpu(), preds.cpu(), average='macro'):.4f}\")\n",
    "\n",
    "# 2. Save Weights (Safetensors)\n",
    "tensor_path = f\"{CONFIG['MODEL_PATH']}.safetensors\"\n",
    "save_file(model.state_dict(), tensor_path)\n",
    "print(f\"üîí Weights saved: {tensor_path}\")\n",
    "\n",
    "# 3. Save Config & Class Mapping\n",
    "json_config = CONFIG.copy()\n",
    "json_config['DEVICE'] = str(json_config['DEVICE'])\n",
    "json_config['NUM_CLASSES'] = num_classes\n",
    "\n",
    "metadata = {\n",
    "    'config': json_config,\n",
    "    'drug_map': drug_map,\n",
    "    'class_mapping': label_encoder.classes_.tolist() # Stores [48, 72, 46...]\n",
    "}\n",
    "\n",
    "json_path = f\"{CONFIG['MODEL_PATH']}_config.json\"\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=4)\n",
    "\n",
    "print(f\"üìú Metadata & Class Mappings saved: {json_path}\")\n",
    "print(\"‚úÖ AushadhiNet Multiclass Upgrade Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5decfd8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7117088937545616"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_labels.cpu(), preds.cpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
