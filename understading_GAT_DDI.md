# Complete Guide: Graph Attention Networks for Drug-Drug Interaction Prediction

This project implements a state-of-the-art Graph Attention Network (GAT) for predicting cardiovascular drug-drug interactions (DDI). The model learns from molecular structures (SMILES) and known interaction patterns to predict interaction types between drug pairs.



## ðŸ“š Part 1: Understanding the Fundamentals

### What is a Graph Neural Network (GNN)?

**Traditional Neural Networks**:
- Work on grid-like data (images, sequences)
- Fixed structure and size
- Example: CNN for images, RNN for text

**Graph Neural Networks**:
- Work on graph-structured data
- Variable structure and size
- Example: Social networks, molecules, drug interactions

### Why Graphs for Drug Interactions?

```
Traditional Approach:
Drug A + Drug B â†’ Interaction? (isolated pairs)

Graph Approach:
Drug A â† connected to â†’ Drug B
  â†“                        â†“
Drug C â† connected to â†’ Drug D
(learns from network patterns)
```

**Advantages**:
1. **Network Effects**: Learns from similar drug pairs
2. **Transitive Relationships**: If A interacts with B, and B with C, what about A and C?
3. **Molecular Similarity**: Similar drugs have similar interaction patterns
4. **Multi-hop Reasoning**: Considers indirect relationships
### How GAT Works for DDI

1. **Node Embedding Learning**:
   - Each drug is represented by its molecular features
   - GAT layers learn to aggregate information from neighboring drugs
   - Attention mechanism focuses on most relevant drug interactions

2. **Attention Mechanism**:
   - Computes attention scores between drug pairs
   - Higher attention = stronger relationship
   - Multi-head attention captures different interaction patterns

3. **Edge Prediction**:
   - Combines embeddings of two drugs
   - MLP predicts interaction type
   - Handles 86 different interaction categories

### Why GAT for DDI?

âœ… **Captures Graph Structure**: Drugs form a network of interactions  
âœ… **Attention Mechanism**: Identifies important drug relationships  
âœ… **Molecular Features**: Leverages chemical structure information  
âœ… **Multi-class Prediction**: Handles multiple interaction types  
---

## ðŸ“– Key Concepts

### Graph Attention Networks (GAT)
- **Attention Mechanism**: Learns importance of neighboring nodes
- **Multi-head Attention**: Captures different relationship types
- **Masked Attention**: Only considers connected nodes
- **Permutation Invariant**: Order of nodes doesn't matter

### Drug-Drug Interactions
- **Pharmacokinetic**: Affects drug absorption, distribution, metabolism, excretion
- **Pharmacodynamic**: Affects drug action at target site
- **Severity Levels**: Minor, moderate, major, contraindicated
- **Clinical Significance**: Impact on patient outcomes

### Molecular Fingerprints
- **Morgan Fingerprints**: Circular fingerprints based on atom neighborhoods
- **Bit Vectors**: Binary representation of molecular features
- **Similarity**: Similar molecules have similar fingerprints
- **Substructure**: Captures presence of chemical motifs

## ðŸ§  Part 2: Understanding Graph Attention Networks

### What is Attention?

**Intuition**: Not all neighbors are equally important.

```
Example: Drug A interacts with:
- Drug B (strong interaction) â†’ High attention
- Drug C (weak interaction) â†’ Low attention
- Drug D (moderate interaction) â†’ Medium attention
```

### How GAT Works

**Step 1: Node Features**
```python
Each drug has features:
- Molecular fingerprint (512 bits)
- Molecular weight
- LogP (lipophilicity)
- H-bond donors/acceptors
- etc.
```

**Step 2: Attention Computation**
```
For each drug pair (i, j):
1. Transform features: h'_i = W * h_i
2. Compute attention score: e_ij = LeakyReLU(a^T [h'_i || h'_j])
3. Normalize: Î±_ij = softmax(e_ij)
```

**Step 3: Aggregation**
```
Update drug representation:
h_i^new = Î£ Î±_ij * h'_j (sum over neighbors)
```

**Step 4: Multi-head Attention**
```
Run attention K times (K heads):
- Head 1: Captures pharmacokinetic interactions
- Head 2: Captures pharmacodynamic interactions
- Head 3: Captures structural similarities
- etc.

Concatenate all heads: h_i^final = [head1 || head2 || ... || headK]
```

---

## ðŸ”¬ Part 3: DDI Dataset

### Dataset Structure

**1. Drug-Drug Interactions (ddis.csv)**
- **File**: `dataset/drugdata/ddis.csv`
- **Total Interactions**: 191,808 drug pairs
- **Unique Drugs**: 1,706 drugs
- **Interaction Types**: 86 different interaction categories
- **Format**: `d1, d2, type, Neg samples`
```
d1        d2        type  Neg samples
DB04571   DB00460   0     DB01579$t
DB00855   DB00460   0     DB01178$t
...
```

- **d1, d2**: Drug pair (DrugBank IDs)
- **type**: Interaction type (0-85, 86 categories)
- **Neg samples**: Negative samples for training

**2. Drug SMILES (drug_smiles.csv)**
- **File**: `dataset/drugdata/drug_smiles.csv`
- **Total Drugs**: 1,706 drugs with SMILES representations
- **Format**: `drug_id, smiles`
- **Coverage**: 100% overlap with DDI data
```
drug_id   smiles
DB04571   CC1=CC2=CC3=C(OC(=O)C=C3C)C(C)=C2O1
DB00855   NCC(=O)CCC(O)=O
...
```

- **drug_id**: DrugBank ID
- **smiles**: Chemical structure representation

### Data Split
- **Training**: 70% (134,265 edges)
- **Validation**: 15% (28,771 edges)
- **Testing**: 15% (28,772 edges)
### Interaction Types

Your dataset has **86 different interaction types**. Examples:
- Type 0: No significant interaction
- Type 1: Minor interaction
- Type 48: Most common (60,751 cases)
- Type 46: Second most common (34,360 cases)
- etc.

**Challenge**: Highly imbalanced classes!
- Some types have 60,000+ examples
- Some types have only 6-10 examples

---

## ðŸ—ï¸ Part 4: Model Architecture Explained

### Overall Pipeline

```
SMILES â†’ Feature Extraction â†’ Graph Construction â†’ GAT â†’ Prediction
```
### Feature Extraction
**Molecular Features from SMILES:**
- **Morgan Fingerprints**: 512-bit circular fingerprints (radius=2)
- **Molecular Descriptors**:
  - Molecular Weight (MW)
  - LogP (lipophilicity)
  - H-bond donors/acceptors
  - Topological Polar Surface Area (TPSA)
  - Rotatable bonds
  - Aromatic rings
  - Fraction of sp3 carbons

**Total Feature Dimension**: 520 (512 + 8)

### Detailed Architecture

**1. Feature Extraction (RDKit)**
```python
Input: SMILES string
       â†“
Parse molecule
       â†“
Extract fingerprint (512 bits)
       â†“
Calculate descriptors (8 values)
       â†“
Output: 520-dimensional vector
```
### GAT Architecture

```
Input: Drug Features (520-dim)
    â†“
GAT Layer 1 (8 heads, 256-dim) + ELU + Dropout
    â†“
GAT Layer 2 (8 heads, 256-dim) + ELU + Dropout
    â†“
GAT Layer 3 (8 heads, 128-dim) + ELU + Dropout
    â†“
Edge Embedding (concatenate source + target)
    â†“
MLP (256 â†’ 128 â†’ 64 â†’ 86)
    â†“
Output: Interaction Type Prediction
```

**2. Graph Construction**
```python
Nodes: 1,706 drugs
Edges: 191,808 interactions (bidirectional = 383,616)
Node features: 520-dim vectors
Edge labels: Interaction types (0-85)
```

**3. GAT Layers**
```python
Layer 1: 520 â†’ 256 (8 heads, concat)
         â†“ ELU + Dropout
Layer 2: 256 â†’ 256 (8 heads, concat)
         â†“ ELU + Dropout
Layer 3: 256 â†’ 128 (8 heads, concat)
         â†“ ELU + Dropout
```

**4. Edge Prediction**
```python
For edge (drug_i, drug_j):
1. Get embeddings: emb_i, emb_j (128-dim each)
2. Concatenate: edge_emb = [emb_i || emb_j] (256-dim)
3. MLP: 256 â†’ 128 â†’ 64 â†’ 86
4. Output: Probability distribution over 86 types
```

---

## ðŸ’» Part 5: Running Your Model

### Step-by-Step Execution

**1. Verify Setup**
```bash
python test_ddi_gat.py
```

Expected output:
```
âœ… ALL TESTS PASSED!
ðŸš€ Ready to train GAT-DDI model!
```

**2. Train the Model**

**Option A: Python Script (Recommended for long training)**
```bash
python GAT_DDI_complete.py
```

**Option B: Jupyter Notebook (Recommended for learning)**
```bash
jupyter notebook GAT_DDI_Prediction.ipynb
```

**3. Monitor Training**
```
EPOCH | TRAIN LOSS | TRAIN ACC | TRAIN F1 | VAL LOSS | VAL ACC | VAL F1
   10 |     2.1234 |    0.6543 |   0.6234 |   2.3456 |  0.6123 | 0.5987
   20 |     1.8765 |    0.7123 |   0.6876 |   2.1234 |  0.6543 | 0.6234
   ...
```

**4. Results**
```
âœ… Training completed in 1234.56 seconds
ðŸŽ¯ Best validation accuracy: 0.7543
ðŸŽ¯ Best validation F1-score: 0.7234

ðŸ† Final Test Accuracy: 0.7456
ðŸ† Final Test F1-Score: 0.7123
```

---

## ðŸ“Š Part 6: Interpreting Results

### Understanding Metrics

**Accuracy**
```
Accuracy = Correct Predictions / Total Predictions

Example: 75% accuracy means:
- Out of 100 drug pairs
- 75 interaction types predicted correctly
- 25 predicted incorrectly
```

**F1-Score**
```
F1 = 2 * (Precision * Recall) / (Precision + Recall)

Better for imbalanced data!
- Considers both false positives and false negatives
- Weighted F1: Accounts for class imbalance
```

**Per-Class Performance**
```
Type 48 (60,751 samples):
- Precision: 0.85 (85% of predictions are correct)
- Recall: 0.92 (92% of actual cases found)
- F1: 0.88

Type 41 (6 samples):
- Precision: 0.20 (only 20% correct)
- Recall: 0.17 (only 17% found)
- F1: 0.18 (poor performance due to few examples)
```

### What Makes a Good Model?

**For Your Research**:
1. **Overall Accuracy > 75%**: Good baseline
2. **Weighted F1 > 0.70**: Handles imbalance well
3. **High Recall for Dangerous Interactions**: Critical for safety
4. **Consistent Performance Across Common Types**: Reliable predictions

---

