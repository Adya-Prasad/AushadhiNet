{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e1c6de1",
   "metadata": {},
   "source": [
    "## Graph Attention Networks v2 (GATv2)\n",
    "GATv2s work on graph data similar to GAT. A graph consists of nodes and edges connecting nodes. For example, in Cora dataset the nodes are research papers and the edges are citations that connect the papers\n",
    "\n",
    "The GATv2 operator fixes the static attention problem of the standard GAT. Static attention is when the attention to the key nodes has the same rank (order) for any query node.\n",
    "\n",
    "The GATv2 operator fixes the static attention problem of the standard GAT. Static attention is when the attention to the key nodes has the same rank (order) for any query node. GAT computes attention from query node $i$ to key node $j$ as,\n",
    "$$e_{ij} = \\text{LeakyReLU}\\left(\\mathbf{a}^\\top [\\mathbf{W}\\vec{h_i}\\Vert\\mathbf{W}\\vec{h_j}]\\right) \\\\ = \\text{LeakyReLU}\\left(\\mathbf{a}_1^\\top\\mathbf{W}\\vec{h_i}+\\mathbf{a}_2^\\top\\mathbf{W}\\vec{h_j}\\right)$$\n",
    "\n",
    "\n",
    "GATv2 allows dynamic attention by changing the attention mechanism\n",
    "\n",
    "$$e_{ij} = \\mathbf{a}^\\top \\text{LeakyReLU} \\left(\\mathbf{W} [h_i \\Vert h_j] \\right) \\\\ = \\mathbf{a}^\\top \\text{LeakyReLU} \\left(\\mathbf{W}_i \\vec{h_i} + \\mathbf{W}_j \\vec{h_j} \\right)$$\n",
    "\n",
    "*   **$e_{ij}$**: The **unnormalized** attention coefficient between node $i$ (query) and node $j$ (neighbor).\n",
    "*   **$\\mathbf{a}$**: A learnable **weight vector** (a shared attention mechanism parameter).\n",
    "*   **$\\mathbf{W}$**: A learnable **weight matrix** that transforms input features.\n",
    "*   **$\\mathbf{W}_i, \\mathbf{W}_j$**: Resulting **sub-matrices** from decomposing $\\mathbf{W}$ when distributed across the concatenated vectors (as shown in the second line of the image).\n",
    "*   **$h_i, h_j$**: The input **feature vectors** for nodes $i$ and $j$, respectively.\n",
    "*   **$\\Vert$**: The **concatenation** operation combining the two feature vectors.\n",
    "*   **$\\text{LeakyReLU}$**: The **non-linear activation function** applied element-wise.\n",
    "*   **$\\top$**: The **transpose** operation.\n",
    "\n",
    "## Dataset\n",
    "Train the model on `dataset/drugdata`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9228ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f2a29bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionV2Layer(nn.Module):\n",
    "    def __init__(self, in_featured:int, out_features:int, n_heads:int, is_concat:bool = True, dropout:float = 0.6, leaky_relu_negative_slope:float = 0.2, share_weights:bool = False):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_concat = is_concat\n",
    "        self.n_heads = n_heads\n",
    "        self.share_weights = share_weights\n",
    "\n",
    "        # calculate the number of dimension per head\n",
    "        if is_concat:\n",
    "            assert out_features % n_heads == 0\n",
    "            # if we are concatenating the multiple heads\n",
    "            self.n_hidden = out_features // n_heads\n",
    "        else:\n",
    "            # if we are averaging the multipls heads\n",
    "            self.n_heads = out_features\n",
    "\n",
    "        # Linear layer for initial source transformation; i.e. to transform the source node embeddings before self-attention \n",
    "        self.linear_1 = nn.Linear(in_features, self.n_hidden * n_heads, bias=False)\n",
    "\n",
    "        # if `share_weights` is true, the same layer is used for the target nodes\n",
    "        if share_weights:\n",
    "            self.linear_r = self.linear_1\n",
    "        else:\n",
    "            self.linear_r = nn.Linear(in_features, self.n_hidden * n_heads, bias=False)\n",
    "\n",
    "        # linear layer to compute attention score e_ij\n",
    "        self.attn = nn.Linear(self.n_hidden, 1, bias=False)\n",
    "\n",
    "        # the activation for the attention score e_ij\n",
    "        self.activation = nn.LeakyReLU(negative_slope=leaky_relu_negative_slope)\n",
    "\n",
    "        # softmax to compute attention a_ij\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # dropout layer to be applied for attention\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def Forward(self, h:torch.Tensor, adj_mat:torch.Tensor):\n",
    "        \"\"\"\n",
    "        h: input node embeddings of shape [n_nodes, in_features] .\n",
    "        adj_mat: adjacency matrix of shape [n_nodes, n_nodes, n_heads]. We use shape [n_nodes, n_nodes, 1] since the adjacency is the same for each head. Adjacency matrix represent the edges (or connections) among nodes. adj_mat[i][j] is True if there is an edge from node i to node j .\n",
    "    \"\"\"\n",
    "        # number of nodes\n",
    "        n_nodes = h.shape[0]\n",
    "\n",
    "        # The initial transformations, for each head. We do two linear transformations and then split it up for each head. \n",
    "        g_l = self.linear_1(h).view(n_nodes, self.n_heads, self.n_hidden)\n",
    "        h_r = self.linear_r(h).view(n_nodes, self.n_heads, self.n_hidden)\n",
    "\n",
    "        ## Calculate attention score\n",
    "        g_l_repeat = g_l.repeat(n_nodes, 1, 1)\n",
    "\n",
    "        g_r_repeat_interleave = g_r.repeat_interleave(n_nodes, dim=0)\n",
    "\n",
    "        # now add the two tensors to get all concat\n",
    "        g_sum = g_l_repeat + g_r_repeat_interleave\n",
    "\n",
    "        # reshapes so that g_sum[i, j] is gl_i + gr_j\n",
    "        g_sum = g_sum(n_nodes, n_nodes, self.n_heads, self.n_hidden)\n",
    "\n",
    "        # calculate e_ij\n",
    "        e = self.attn(self.activation(g_sum))\n",
    "\n",
    "        # remove the last dimension of size 1\n",
    "        e = e.squeeze(-1)\n",
    "\n",
    "        # validating adjacency matrix shape: [n_nodes, n_nodes, n_heads] or[n_nodes, n_nodes, 1]\n",
    "        assert adj_mat.shape[0] == 1 or adj_mat.shape[0] == n_nodes\n",
    "        assert adj_mat.shape[1] == 1 or adj_mat.shape[1] == n_nodes\n",
    "        assert adj_mat.shape[2] == 1 or adj_mat.shape[2] == self.n_heads\n",
    "\n",
    "        # Mask e_ij based on adjacency matrix e_ij is set to -inf if there is no edge from i to j\n",
    "        e = e.masked_fill(adj_mat == 0, float('-inf'))\n",
    "\n",
    "        # normalized attention scores (or coefficients)\n",
    "        a = self.softmax(e)\n",
    "\n",
    "        # apply dropout regularization\n",
    "        a = self.dropout(a)\n",
    "\n",
    "        # calculate final output for each head\n",
    "        attn_res = torch.einsum('ijh, jhf->ihf', a, g_r)\n",
    "\n",
    "        # concatenate the heads\n",
    "        if self.is_concat:\n",
    "            return attn_res.reshape(n_nodes, self.n_heads * self.n_hidden)\n",
    "        else:\n",
    "            # take the mean of the heads\n",
    "            return attn_res.mean(dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
